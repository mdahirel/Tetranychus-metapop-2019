---
title: Analysis of replicated *Tetranychus* metapopulation experiment - abundance and
  temporal variability
author: "Stefano Masier, Maxime Dahirel, Frederik Mortier, Dries Bonte (this code by M. Dahirel and S. Masier)"
date:
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
editor_options:
  chunk_output_type: console
bibliography: tetranychus-metapop-2019-refs.bib
csl: journal-of-animal-ecology.csl
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE)
```


# A brief introduction

(see manuscript/preprint text for more context)

This script analyses data resulting from the study of experimental metapopulations of spider mites *Tetranychus urticae*. Each replicate metapopulation was composed of 9 patches connected by bridges; the bridge length varied between replicates. Abundances of adult females were tracked weekly in each patch of each metapopulation. We are interested in the effect of bridge length and randomization treatment (see text) on mean patch-level population size, mean metapopulation size and various variability metrics. 

This script relies heavily on de Villemereuil *et al.* [-@villemereuilGeneralMethodsEvolutionary2016] (for converting latent means and variance-covariance matrices to data/observed scale) and Wang and Loreau [-@wangEcosystemStabilitySpace2014] (for the principle and formulas of $\alpha$, $\beta$, $\gamma$ variability in metapopulations). Everything below assumes the reader has read these two papers.

# Part 1 : Preparation

## 1A - packages

First, we’re going to load all the packages we need. This includes the `tidyverse` packages [@wickhamWelcomeTidyverse2019], for data wrangling at the beginning and summarising/plotting at the end, as well as the packages needed to do Bayesian inference and post-processing of Bayesian model results.
This script uses the Stan language [@carpenterStanProbabilisticProgramming2017] for model fitting (we can use the `rstan` or the `cmdstanr` implementation; see https://mc-stan.org/ for how to install them). The `brms` package [@burknerBrmsPackageBayesian2017] allows you to write a wide range of Stan models using R syntax; it then does the “translation” before fitting. The `QGglmm` package by de Villemereuil *et al.* [-@villemereuilGeneralMethodsEvolutionary2016] is useful to convert latent scale variance-covariance matrices from a GLMM output to observed scale matrices, needed for our analysis.

```{r packages-loading}
#library(rstan) ## Stan backend
library(cmdstanr) ## Stan backend (another)
library(brms) ## the interface we are using


library(tidyverse)
library(bayesplot)
library(tidybayes)
library(matrixStats)

library(QGglmm) ## this package is needed to help convert variance-covariance matrices from latent to data scale

library(patchwork) #plotting

library(here)

## some useful default settings
# rstan_options(auto_write = TRUE) #for rstan
options(mc.cores = 4) ## reduce/increase depending on cores available
N_chains <- 4
N_warmup <- 2000   ## used for publication: 2000 ## 200 is enough for tests
N_iter <- N_warmup + 2000  ## recommended for publication-level quality: 2000 iterations post warmup when all chains combined 
## Nwarmup + 200 is probably good enough for tests

```

**Note:** The finished model object will likely be large (at least several 100s Mb) and take some time to fit (a few hours to half a day on the few laptops it has been tested on). If this causes problems, we invite you to try a model with a much smaller `N_warmup` and number of iterations post-warmup (see comments in code chunk above). It should be close enough to convergence, despite warnings, to allow you to get a general idea of the results.


## 1B – data loading and wrangling

```{r data-load}
raw_data <- read_csv(here("data","tetranychus-metapop-2019-dataset.csv"))
```

The variables in `raw_data` are as follow:

- `METAPOP_ID`: unique ID for each replicate metapopulation
- `LENGTH`: length of bridges between patches in the metapopulation (4, 8, or 16 cm)
- `SHUFFLE`: within-metapopulation reshuffling treatment. NO: control, no reshuffling, R: reshuffling
- `REPLICATE`: replicate metapopulation number within a treatment set. The `METAPOP_ID` string corresponds to "`LENGTH` _ `SHUFFLE` _ `REPLICATE`"
- `PATCH`: Patch location in the 3 by 3 metapopulation (written as "row.column")
- `WEEK`: weeks of data collection (first week coded as 1)
- `AFEMA`: number of adult females counted on patch, our measure of abundance

In the raw dataset, data are stored in the long format (one row = one patch observation at one moment of time). We're going to temporarily switch data to the wide format (one row = snapshot of an entire replicate at one moment in time, with one column by patch), so that it's easier to do a few things:

- a few replicates have NAs on *some* patches on one week or so, for some reason (transcription error?). To be safe, let's remove the entire week for the metapop in question, even if most patches were recorded.

- we're going to need a variable representing the total metapopulation size for each replicate and week, and it's just easier to do it from the wide table

```{r data-reshape}

data_wide <- raw_data %>% 
  mutate(PATCH= paste("P", PATCH, sep = "")) %>% 
  mutate(PATCH= str_remove(PATCH, "[.]")) %>% 
### the two lines above make patches names (a) easier to use as column names (letter as 1st character)
### and consistent with brms standards on response variable names (best to avoid dots and underscore,
### as some functions will remove them for output names and then matching input and output becomes slightly harder)
  mutate(LENGTH = fct_recode(factor(LENGTH),
                             `16 cm (low)` = "16",
                             `8 cm (medium)` = "8",
                             `4 cm (high)` = "4"),
         SHUFFLE = fct_recode(as.factor(SHUFFLE), 
                              `control`="NO",
                              `randomized`="R")) %>% 
  pivot_wider(names_from = PATCH, values_from = AFEMA) %>%
  drop_na() #3 out of ~550 rows contain NAs in at least one patch, discard

```

From here, we create two tables. One for the patch-by-patch model, where we switch back to the long format. One for a model where metapopulation size is the response, where we keep the table in its current format and just create a sum column.

```{r data-patch}
P_data <- data_wide %>% 
  pivot_longer(P11:P33, names_to = "PATCH", values_to = "AFEMA") %>% 
  mutate(local_connectedness = 1 + 1 * (PATCH %in% c("P12","P21","P23","P32"))+ 
                               2 * (PATCH %in% c("P11","P13","P31","P33"))) %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "1", 
                                          `side (medium)` = "2", 
                                          `corner (low)`="3")) 

## adding dummy centered variables
P_data <- P_data %>% 
  mutate(is.randomized = as.numeric(SHUFFLE =="randomized") - mean(as.numeric(SHUFFLE =="randomized"))) %>% 
  mutate(is.local5 = as.numeric(local_connectedness =="side (medium)") - mean(as.numeric(local_connectedness =="side (medium)")),
         is.local3 = as.numeric(local_connectedness =="corner (low)") - mean(as.numeric(local_connectedness =="corner (low)")),
         is.landscape8 = as.numeric(LENGTH =="8 cm (medium)") - mean(as.numeric(LENGTH =="8 cm (medium)")),
         is.landscape16 = as.numeric(LENGTH =="16 cm (low)") - mean(as.numeric(LENGTH =="16 cm (low)")))
```


```{r data-metapop}
M_data <- data_wide %>% 
  mutate(METAPOPSUM = select(.,P11:P33) %>% rowSums()) %>% 
  select(METAPOP_ID,LENGTH,SHUFFLE,REPLICATE,WEEK,METAPOPSUM)%>% 
  mutate(LENGTH = factor(LENGTH))

## adding dummy centered variables
M_data <- M_data %>% 
  mutate(is.randomized = as.numeric(SHUFFLE =="randomized") - mean(as.numeric(SHUFFLE =="randomized"))) %>% 
  mutate(is.landscape8 = as.numeric(LENGTH =="8 cm (medium)") - mean(as.numeric(LENGTH =="8 cm (medium)")),
         is.landscape16 = as.numeric(LENGTH =="16 cm (low)") - mean(as.numeric(LENGTH =="16 cm (low)")))

```

(from here onwards, anything prefixed by `P_` refers to the patch-level analysis, anything prefixed by `M_` to the metapopulation-level analysis. Some things will be prefaced by `P_M_`; these will be metapop-level things estimated from the patch-level model).

## 1-C Functions for $\alpha$, $\beta$, $\gamma$ variability

One of the aims of this script is to estimate the $\alpha$, $\beta$ and $\gamma$ variability metrics proposed by Wang and Loreau [-@wangEcosystemStabilitySpace2014], while accounting for among metapopulation-variability. The short functions below calculate these metrics, given as input a temporal variance-covariance matrix, and a vector of mean patch population sizes (both must be numeric and contain the same number of patches, of course)(and also both should be on the *observed/data* scale in the case of a GLMM).

```{r wang-loreau}

alpha_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_L = sum(sqrt(diag(varcorr))) / sum(means)
  return(CV_L^2)
}

gamma_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_M = sqrt(sum(varcorr)) / sum(means)
  return(CV_M^2)
}

# no function for beta 1, it's simply 1/phi or alpha/gamma
# the beta 2 and the phi functions are here for completeness and checks, we don't need them
# because both betas can be defined through alpha and gamma
# beta 1: see above, beta2 = alpha - gamma

phi_wang_loreau <- function(varcorr) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
  if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
  if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    
  return(sum(varcorr) / (sum(sqrt(diag(varcorr))))^2)
}

beta2_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  sqrt_w_bar <- sum(sqrt(diag(varcorr))) / length(means)
  return((sqrt_w_bar^2 - (sum(varcorr)) / (length(means)^2)) / mean(means)^2)
}

uneven_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  sqrt_w_bar <- sum(sqrt(diag(varcorr))) / length(means)
  
  a <- (means - mean(means))^2
  b <- (sqrt(diag(varcorr)) - sqrt_w_bar)^2
  c <- (length(means) * mean(means)^2)
  return(sum(a + b)/c)
}


```

# Part 2 - Fitting the model

## 2A - Models description

### Patch-level model

We fit a generalized linear mixed/multilevel model to the abundance data. This Poisson model (because count data) includes "random" effects of metapopulation ID (to account for the fact some replicates may have higher/lower average population sizes) and patch nested in ID (because patches may differ beyond the effect of treatment, local connectedness and replicate). Importantly, it also includes a time random effect, to account for temporal patch variance-covariance.
You can see below that we are estimating a separate temporal variance-covariance matrix for each replicate $i$. This means that (i) each patch has its own temporal variance, and patches form the same replicate can be correlated. We will average everything downstream as needed, but given the non-linearities everywhere in a GLMM, and the fact patch names are arbitrary (we could have rotated metapopulations without changing anything meaningful), it is probably best for the variabilities to calculate everything replicate by replicate first and only average later. It is important to note that the presence of the temporal matrices it means this is a model with observation-level random effects [@harrisonUsingObservationlevelRandom2014], so no further overdispersion to worry about.

Intuition behind this formulation (latent residuals via OLRE + specified covariance structure) is the same as in this comment by Paul Bürkner (https://github.com/paul-buerkner/brms/issues/600#issuecomment-511677732) 

The formula for the model for the number of adult females $N_{i,x,y,t}$ in metapopulation $i$, in the patch of coordinates $x,y$ at time $t$ is

$$
N_{[i,x,y,t]} \sim  {\textrm{Poisson}}(\lambda_{[i,x,y,t]}),
$$
$$
\log(\lambda_{[i,x,y,t]}) = \beta_{0} + \sum_{j}{\beta_{j}x_{i,j,x,y}} + \alpha_{[i]} + \gamma_{[i,x,y]} + \eta_{[i,x,y,t]},
$$
$$
\alpha_{[i]} \sim \mathrm{Normal}(0, \sigma_{\alpha}),
$$
$$
\gamma_{[i,x,y]} \sim \mathrm{Normal}(0, \sigma_{\eta}),
$$
$$
\begin{bmatrix} \eta_{[i,1,1,t]} \\ ... \\ \eta_{[i,3,3,t]} \end{bmatrix} 
\sim 
\textrm{MVNormal}
\begin{pmatrix}
\begin{bmatrix} 0 \\ ... \\ 0  \end{bmatrix} ,
\boldsymbol{\Omega}_{[i]}
\end{pmatrix},
$$
$$
\boldsymbol{\Omega}_{[i]} = 
\begin{bmatrix}
\sigma_{\eta[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{\eta[i,3,3]} 
\end{bmatrix}
\boldsymbol{R}_{[i]}
\begin{bmatrix}
\sigma_{\eta[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{\eta[i,3,3]} 
\end{bmatrix}
$$

where $\beta$ are the fixed effects (with $\beta_{0}$ the intercept), $\alpha$ are replicate/metapopulation random effects, $\gamma$ patch-level random effects, and $\eta$ temporal abundance fluctuations (not to be confused with $\alpha$, $\beta$, $\gamma$ variabilities). $\boldsymbol{\Omega}_{[i]}$ is the temporal covariance matrix for the replicate $i$ and $\boldsymbol{R}_{[i]}$ the corresponding correlation matrix. For implementation, we transform the treatment covariates into dummy centred variables following schielzethSimpleMeansImprove2010, this has the added benefit of making $\beta_{0}$ the intercept of the "average" treatment.

### Meta-population model

Similarly, we can write a (much simpler) model for the total metapopulation size (total number of adult females) $M$. Because there are no correlations to worry about, we can use a negative binomial model here to model the within-replicate temporal variation:

$$
M_{[i,t]} \sim  {\textrm{NegBinomial}}(\lambda_{[i]}, \phi{[i]}),
$$
$$
\log(\lambda_{[i]}) = \beta_{0} + \sum_{j}{\beta_{j}x_{i,j}} + \gamma_{[i]},
$$
$$
\log(\phi_{[i]}) = \alpha_{0} + \sum_{j}{\alpha_{j}x_{i,j}} + \eta_{[i]},
$$
$$
\gamma_{[i]} \sim \mathrm{Normal}(0, \sigma_{\gamma}),
$$
$$
\eta_{[i]} \sim \mathrm{Normal}(0, \sigma_{\eta}).
$$  

Here $\gamma$ and $\eta$ refer to the metapopulation-level random effects, for the mean parameter and for the shape parameter, respectively. Similarly, $\beta$ and $\alpha$ refer to the fixed effects coefficients for the mean and the shape.


## 2B - Implementation - formulas

### Patch-level model

```{r P_formulas}
P_formula <- bf(
  AFEMA~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+
    (1|METAPOP_ID/PATCH)+(0+PATCH|gr(METAPOP_ID:WEEK,by=METAPOP_ID)),
  family=poisson)
```

### Metapopulation model

```{r M_formulas}
M_formula  <- bf(
  METAPOPSUM~(is.landscape8 + is.landscape16)*is.randomized+(1|METAPOP_ID),
  nlf(shape~1/exp(loginvshape)), 
  ##we fit the model on the inverse of shape 
  ##(avoid assuming a priori very large overdispersions are more likely) 
  ## and we need a log transformation to keep things >0
  loginvshape~(is.landscape8 + is.landscape16)*is.randomized+(1|METAPOP_ID),
  family=negbinomial(link_shape = "identity"))

```

## 2C - Implementation - choosing priors

We then select our priors. Because the patch-level model is complex, with lots of moving parts relative to the amount of data, we can't just use "usual" weakly informative priors; we have to think a little harder to avoid a prior pushing the model towards blatantly wrong extreme values. Luckily we have independent prior data from the same species/host plant system in De Roissart *et al.* [-@deroissartDataSpatialSpatiotemporal2016; -@deroissartSpatialSpatiotemporalVariation2015], so we're going to be able to rely on explicit prior knowledge

```{r import_prior_data}
prior_data <- read_csv(here("data","prior_data.csv"))
```

(This is a copy of the relevant columns in the Dryad data in De Roissart *et al.* [-@deroissartDataSpatialSpatiotemporal2016], see there and README for details).

Densities there are given in mites per cm^2 (our own patches are 25cm^2). We're going to use the adult female column `AvgFem`:

```{r filter_prior_data}
prior_data <- prior_data %>% 
  group_by(MP,P,Treatment,Date) %>% 
  mutate(total=AvgFem+AvgEggs+AvgMal+AvgJuv) %>% ## all the mites on the patch at time t
  group_by(MP,P,Treatment) %>%  ##metapop plus patch ID uniquely define a patch
  mutate(was_populated = mean(total)>0) %>% ## we exclude all patches that were NEVER populated
  ungroup() %>% 
  filter(was_populated==TRUE)
```

Now we're going to use these data to get very very rough prior info
```{r distri_prior_data}

log(median(prior_data$AvgFem*25))   #the \mu of a lognormal with same observed median as prior data
log(median(prior_data$AvgFem*25*9))

log(1 + var(prior_data$AvgFem)/mean(prior_data$AvgFem)^2) #the \sigma^2 of a lognormal with same observed mean and variance
```

OK, how does this help set us reasonable priors?

First, there are five types of parameters:

- intercept for the fixed effect latent mean
- random effect standard deviation
- intercepts for the shape parameter (Metapop-model only)
- fixed effects coefficients
- correlation matrix

For the last three, we're just going to use general purpose weakly informative priors as in [@mcelreathStatisticalRethinkingBayesian2020]. But we're going to need better for the other two. A good but broad prior for the fixed effect latent mean would be "any value within the distribution of observed prior values" (so Normal(3, 1) would work for the patch-level, Normal(5,1) for the metapop).

For the random effect SDs, one can use the total log-scale variance in prior data as a guide, so roughly 1. The sum of replicate level variance, patch-level variance and within-patch level variance should then be around 0.9-1. So the typical prior variance for each of the 3 components should be around 0.3, leading to prior SDs around 0.5-0.55 as a pretty typical value.

(note: specifying larger priors is not dramatic and does not change outcome of among-treatment comparisons, but tends to shift all predicted means on the observed scale a bit higher than what is actually observed.)

```{r prior-setting}
P_prior <- c(
    ### PRIORS FOR THE PATCH-LEVEL MODEL
    set_prior("normal(2.8,1)", class = "Intercept"),
    set_prior("normal(0,1)", class = "b"),
    set_prior("normal(0,0.5)", class = "sd"),
    set_prior("lkj(2)", class = "cor") # mildly skeptical of very high correlations (positive or negative)
  )

M_prior <- c(
      ### PRIORS FOR THE METAPOP LEVEL MODEL
    set_prior("normal(5,1)", class = "Intercept"),
    set_prior("normal(0,1)", class = "b"),
    set_prior("normal(0,1)", class="b",nlpar="loginvshape"),
    set_prior("normal(0,1)", class = "sd", nlpar="loginvshape"),
    set_prior("normal(0,0.5)", class = "sd")
)
```

## 2D - Implementation - fitting

We can now fit the models based on the choices we made earlier

```{r P_model_fitting}
if(file.exists(here("R_output","P_mod.Rdata"))){
  # this if-else statement is avoid re-fitting a model if there is already one existing in R_output
  # to override, re-run the model and re-save manually by selecting only the relevant code lines (or delete the Rdata object before relaunching this code chunk)
  load(here("R_output","P_mod.Rdata"))
  }else
    {

P_mod <- brm(formula=P_formula, prior=P_prior, data = P_data, 
           iter=N_iter,warmup=N_warmup,chains=N_chains,
           seed=42, control=list(adapt_delta=0.9,max_treedepth=15),
           backend="cmdstanr")
save(list=c("P_mod"), file=here("R_output","P_mod.Rdata"))
}
```


```{r M_model_fitting}
if(file.exists(here("R_output","M_mod.Rdata"))){
  load(here("R_output","M_mod.Rdata"))
  }else{
M_mod <- brm(formula=M_formula, prior=M_prior, data = M_data, 
           iter = N_iter, warmup = N_warmup, chains = N_chains, seed = 42,
           control = list(adapt_delta = 0.9),
           backend= "cmdstanr")

save(list=c("M_mod"), file=here("R_output","M_mod.Rdata"))
}

## with rstan as backend, the model *may* throw some some rhats=NA warnings on exit
## they can (for once) be safely ignored: related issue: https://github.com/paul-buerkner/brms/issues/865
## these warnings don't pop up when cmdstanr is used as backend 
## (other "normal" warnings will appear if needed (e.g during a test run with low iters) with both backends)
## or when doing summary(mod)
## you can check that only parameters that should be constant give NA warnings through rhat(mod)
## and quickly look at the rhats:
## hist(rhat(mod));abline(v=1.01,col="red")
```

Note that it can take some time to finish (around 2000-4000 iterations/h/core on my laptop).

```{r model_summary}
summary(P_mod)  #look at Rhat and ESS both tail and bulk
```

You also need to check that the model can reproduce the data well, through posterior predictive checks [@gabryVisualizationBayesianWorkflow2019]
(here, given we have observation-level random effects, it should be OK)

```{r model_ppchecks}
pp_check(P_mod, nsamples = 20)
## can the model generate the observed data? (there are many ppcheck possibles, see the help of the bayesplot package)
## for P_mod, basic pp_checks will return perfect fits, since there are obs-level random effects
```

## 2E - couldn't the patch-level model be simpler?

One may wonder why go that far for the patch-level model. Surely one could simply use a negative binomial model similar to the metapopulation-level one, just one level lower. Not quite. First, we can't gather the variables needed to estimate alpha, beta and gamma variabilities from such a model. And second, such a model assumes that within-patch temporal variation is stochastic, and ignores across-patch temporal covariation. Let's have a quick look:

```{r P_model_fitting_alternate}
### adapted formula
P_formula2 <- bf(
  AFEMA~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+
    (1|METAPOP_ID/PATCH),
  nlf(shape~1/exp(loginvshape)),
  loginvshape~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+
      (1|METAPOP_ID/PATCH),
  family=negbinomial(link_shape="identity"))

### adapted priors
P_prior2 <- c(
  ### UPDATED PRIORS FOR THE PATCH LEVEL MODEL
  set_prior("normal(2.8,1)", class = "Intercept"),
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class="b",nlpar="loginvshape"),
  set_prior("normal(0,1)", class = "sd", nlpar="loginvshape"),
  set_prior("normal(0,0.5)", class = "sd")
)

### adapted model
if(file.exists(here("R_output","P_mod_alt.Rdata"))){
    load(here("R_output","P_mod_alt.Rdata"))
  }else{
P_mod2 <- brm(formula=P_formula2, prior=P_prior2, data = P_data, 
             iter=N_iter,warmup=N_warmup,chains=N_chains,
             seed=42, control=list(adapt_delta=0.9,max_treedepth=15),
             backend="cmdstanr")
save(list=c("P_mod2"), file=here("R_output","P_mod_alt.Rdata"))
  }

### let's build some residuals (ratio between observed values and predicted patch-level means)
P_data$alt_residuals=(P_data$AFEMA/fitted(P_mod2, re_formula=~(1|METAPOP_ID/PATCH))[,1])

ggplot(P_data) +
  geom_boxplot(aes(factor(WEEK),alt_residuals))+
  facet_wrap(~METAPOP_ID)+geom_hline(yintercept=1,col="red")+
  coord_cartesian(ylim=c(0,4)) #cropping outlier dots to better see the boxes
## clear evidence of week-to-week metapopulation-wide fluctuations that are not accounted for by the negative binomial model

ggplot(P_data) +
  geom_boxplot(aes(factor(PATCH),alt_residuals))+
  facet_wrap(~METAPOP_ID)+geom_hline(yintercept=1,col="red")+
  coord_cartesian(ylim=c(0,4))
## while patch-to-patch variations are well accounted for, as one would expect from model structure

```

It's fairly easy to see that the negative binomial model for patches clearly misses the fact that the patches in a given metapopulation vary somewhat synchronously from week to week, and is thus incomplete.

# Part 3 - Obtaining key metrics from model output

## Extraction

Once we have fitted and chosen a model, it's time to extract what we need from it. 

There's going to be a lot of variables, so we're gonna keep following our naming convention to keep things clear:

- columns or objects prefixed by `P_` refer to the patch by patch model
- ...prefixed by `M_` to the metapop-level model

### Patch-level model

Let's start by extracting the total temporal covariance matrix

```{r}
memory.limit(40000) #### if needed, a little memory boost above defaults to handle the full vcv matrices in final model

P_full_vcv <- VarCorr(P_mod, summary = FALSE)
```


```{r}
P_sd_metapop<-P_full_vcv$`METAPOP_ID`$sd

P_sd_patch<-P_full_vcv$`METAPOP_ID:PATCH`$sd

P_vcv_time_global <- P_full_vcv$`METAPOP_ID:WEEK`$cov

rm(P_full_vcv) ## remove as VERY large and not needed anymore
gc() #garbage collection


```


```{r}
P_vcv_time_latent<-P_data %>% 
  select(METAPOP_ID,LENGTH,SHUFFLE) %>% 
  expand_grid(.iteration = 1:(N_chains * (N_iter-N_warmup))) %>% 
  distinct() %>% 
  mutate(P_vcv_time_latent=map2(.x=.iteration,.y=METAPOP_ID,
                                .f=function(iter=.x,metapop=.y,source=P_vcv_time_global){
                                  include <- colnames(source)
                                  include <- include[str_detect(include, pattern = metapop)]
                                  return(source[iter,include,include])
                                }))
```

and the corresponding latent_intercepts

```{r}
P_latent_intercepts <- P_data %>% 
  select(METAPOP_ID,SHUFFLE,LENGTH,local_connectedness,PATCH, 
         is.local5, is.local3, is.landscape8, is.landscape16, is.randomized) %>% 
  distinct() %>% 
  add_fitted_draws(P_mod,re_formula=~(1|METAPOP_ID/PATCH),scale="linear") %>% 
  ungroup() %>% 
  select(METAPOP_ID,SHUFFLE,LENGTH,PATCH,.iteration=.draw,.value) %>% 
  pivot_wider(values_from=.value,names_from=PATCH) %>% 
  group_by(METAPOP_ID,SHUFFLE,LENGTH,.iteration) %>% 
  nest(data=c(P11,P12,P13,P21,P22,P23,P31,P32,P33)) %>% 
  rename(P_latent_intercept="data") %>% 
  mutate(P_latent_intercept=map(.x=P_latent_intercept,
                                .f=~.x %>% unlist()))
```

```{r merge-coefficients-tables}
P_tab <- P_latent_intercepts %>% 
  left_join(P_vcv_time_latent) 

P_vars<-tibble(.iteration=1:8000,
               P_var_metapop=P_sd_metapop[,1]^2,
               P_var_patch=P_sd_patch[,1]^2)

P_tab<-P_tab %>% 
  left_join(P_vars)
```

## Estimation of key quantities

We first use information on patch-level fixed effects and patch-level temporal variances to estimate data-scale patch-level means. Because we allow within-patch variance to vary among replicates (and so among treatments), we can't infer anything about *mean* differences among treatments from fixed effects only, as the data-scale mean depends on both latent mean and latent within-patch variance [see @villemereuilGeneralMethodsEvolutionary2016]. So we need to first estimate patch means from latent means and variances, and then average across patches within replicates:

```{r P_means}
P_tab <- P_tab  %>%  
  mutate(P_pred = map2( ## patch by patch patch-level average
    .x = P_latent_intercept, .y = P_vcv_time_latent,
    .f = ~ exp(.x + diag(.y)/2)  ## analytic form for the Poisson model, see annex villemereuil
  )) %>% 
  ### we then average within metapops:
  mutate(P_mean_all = map(.x = P_pred, .f = ~mean(.x))) %>% 
  mutate(P_mean_corners = map(.x = P_pred, .f = ~mean(.x[c(1,3,7,9)]))) %>% 
  mutate(P_mean_center = map(.x = P_pred, .f = ~mean(.x[c(5)]))) %>% 
  mutate(P_mean_sides = map(.x = P_pred, .f = ~mean(.x[c(2,4,6,8)]))) %>%
  unnest(c(P_mean_all, P_mean_corners,P_mean_center,P_mean_sides)) 
```

Now, to estimate $\alpha$, $\beta$ and $\gamma$ variabilities, we need to get the temporal variance-covariance matrices from the latent scale to the data scale [see @villemereuilGeneralMethodsEvolutionary2016]. Ideally we'd use the `QGmv...` functions in the `QGglmm` package, but they become terribly inefficient in memory use once the multivariate model has more than 5 variables (and we have 9). It's because the multivariate functions integrate with `cubature` rather than using the closed form version, (if I understood correctly) because they have to work even for cases where the different variables are from different families. But we're lucky because our 9 variables have the same family, so it's actually very easy to do it manually using the closed form formulas in the source papers!! 

The first step is to estimate a $\psi$ matrix, a diagonal matrix containing for each variable its $\psi$ value. And actually, we've already estimated it, since for a Poisson model, $\psi$ is the predicted mean (i.e. `P_pred` for us). So we just have to diagonalise it. Then we can use it to convert the latent scale temporal VCV to the observed scale, and finally use these VCVs and the predicted means to estimate the $\alpha$, $\beta$ and $\gamma$ variabilities:

```{r P_variabilities}
P_tab <- P_tab %>% 
  mutate(P_psi = map(.x = P_pred, .f = function(.x){diag(.x)})) %>% 
  mutate(P_vcv_time_obs = map2(.x = P_psi, .y= P_vcv_time_latent,
                                .f = ~ (.x %*% .y %*% t(.x))
  )) %>% 
  mutate( 
    P_alpha = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% alpha_wang_loreau(varcorr=., means = .y)),
    P_gamma = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% gamma_wang_loreau(varcorr=., means = .y)),
    P_uneven = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% uneven_wang_loreau(varcorr=., means = .y))
  ) %>% 
  unnest(cols=c(P_alpha,P_gamma,P_uneven)) %>% 
  mutate(
    P_beta1 = P_alpha / P_gamma, P_phi = P_gamma/P_alpha, P_beta2 = P_alpha - P_gamma
  )
```

```{r for_export}

P_tab_plots <- P_tab %>% ungroup() %>% select(-c(P_vcv_time_latent, P_latent_intercept, P_pred, P_psi, P_vcv_time_obs))

write_csv(x = P_tab_plots, file = here("R_output","P_tab.csv"))
```



# Part 4- Figures [BIG WORK IN PROGRESS]

Before doing the figures, we need to create color palettes that'll be used throughout:

```{r}
P_tab_plots <- read_csv(here("R_output","P_tab.csv")) %>% 
  mutate(
    LENGTH=fct_relevel(
      factor(LENGTH),
      "16 cm (low)","8 cm (medium)","4 cm (high)")
  )
```


```{r figure-palettes}
paletteLENGTH <- c("#D55E00", "#E69F00", "#F0E442") #for figures 2 to 4
paletteLOCAL <-  c("#e66101", "#fdb863", "#5e3c99") #for figure 5
```

And we need to summarise the observed values to display them along the posterior when relevant:

```{r obs}
P_obsmeans <- P_data %>%
  group_by(SHUFFLE,LENGTH,METAPOP_ID,PATCH, local_connectedness) %>%
  summarise(P_mean=mean(AFEMA), P_se=plotrix::std.error(AFEMA)) %>% 
  ungroup()%>% 
  mutate(
    LENGTH=fct_relevel(
      factor(LENGTH),
      "16 cm (low)","8 cm (medium)","4 cm (high)"),
    local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )
  )
```

## patch level means

```{r P_popsize}
p1<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=SHUFFLE,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60",
                     height=0.2,size=3) +
  #geom_errorbarh(data=P_obsmeans %>% group_by(SHUFFLE) %>% summarise(mean=mean(P_mean)),
  #               aes(y=SHUFFLE,xmin=mean,xmax=mean),height=.8,size=1,col="grey60")+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_x_continuous("")+
  scale_y_discrete("Randomization?")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p2 <- P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_mean_all))  %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=LENGTH,alpha=(1/(P_se^2/P_mean^2))),col="grey60",
                     height=0.2,size=3) +
  #  geom_errorbarh(data=P_obsmeans %>% group_by(LENGTH) %>% summarise(mean=mean(P_mean)),
  #               aes(y=LENGTH,xmin=mean,xmax=mean),height=1,col="grey60")+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("")+
  scale_y_discrete("Metapopulation connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p3 <- P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "P_mean_center", 
                                          `side (medium)` = "P_mean_sides", 
                                          `corner (low)`="P_mean_corners")) %>%
  mutate(local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )) %>% 
  group_by(.iteration, local_connectedness) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=local_connectedness,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60", height=0.2,size=3) +
  #  geom_errorbarh(data=P_obsmeans %>% group_by(local_connectedness) %>% summarise(mean=mean(P_mean)),
  #               aes(y=local_connectedness,xmin=mean,xmax=mean),height=1,col="grey60")+
  stat_halfeye(aes(x=mean,y=local_connectedness, fill=local_connectedness),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLOCAL)+
  scale_x_continuous("mean patch population size (adult females)")+
  scale_y_discrete("Local connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p1/p2/p3

```


```{r}
P_tab_plots %>% 
  group_by(.iteration, SHUFFLE)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE, fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH, fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (8 links)` = "P_mean_center", 
                                          `side (5 links)` = "P_mean_sides", 
                                          `corner (3 links)`="P_mean_corners")) %>% 
  group_by(.iteration, local_connectedness) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
    ungroup() %>% 
  compare_levels(variable=mean, by=local_connectedness, fun=`/`) %>% 
  mean_hdi()
```



```{r P_vars}
p1<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_x_continuous("")+
  scale_y_discrete("Randomization?")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.8,1.4))

p2<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_alpha))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean ", alpha, " variability")))+
  scale_y_discrete("Metapopulation connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.8,1.4))

p3<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.20,0.35))

p4<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_phi))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean spatial synchrony (", phi1, " = ", 1/beta,")")))+  #phi1 to get the cursive
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.20,0.35))

p5<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.15,0.45))

p6<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_gamma))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean ", gamma, " variability")))+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.15,0.45))

p7<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+
  coord_cartesian(xlim=c(0,1.5))

p8<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_uneven))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("mean spatial unevenness")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0,1.5))

(p1/p2)|(p3/p4)|(p5/p6)|(p7/p8)

```

```{r}
P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()
```

```{r P_size_inter}
p1<-P_tab_plots %>% 
  group_by(.iteration, LENGTH, SHUFFLE) %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=LENGTH,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60",height=0.2,size=3) +
  #geom_errorbarh(data=P_obsmeans %>% group_by(SHUFFLE,LENGTH) %>% summarise(mean=mean(P_mean)),
  #               aes(y=LENGTH,xmin=mean,xmax=mean),height=.8,size=1,col="grey60")+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("")+
  scale_y_discrete("Metapopulation connectedness")+
  facet_wrap(~SHUFFLE) +
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p2 <- P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "P_mean_center", 
                                          `side (medium)` = "P_mean_sides", 
                                          `corner (low)`="P_mean_corners")) %>% 
    mutate(local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )) %>% 
  group_by(.iteration, local_connectedness, SHUFFLE) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=local_connectedness,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60",height=0.2,size=3) +
  #geom_errorbarh(data=P_obsmeans %>% group_by(SHUFFLE,local_connectedness) %>% summarise(mean=mean(P_mean)),
  #               aes(y=local_connectedness,xmin=mean,xmax=mean),height=.8,size=1,col="grey60")+
  stat_halfeye(aes(x=mean,y=local_connectedness, fill=local_connectedness),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5)+
  scale_fill_manual(values=paletteLOCAL)+
  scale_x_continuous("mean patch population size (adult females)")+
  scale_y_discrete("Local connectedness")+
  facet_wrap(~SHUFFLE) +
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p1/p2
```



# References




