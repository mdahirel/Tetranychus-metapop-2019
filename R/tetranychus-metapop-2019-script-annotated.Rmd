---
title: Analysis of replicated *Tetranychus* metapopulation experiment - abundance and
  temporal variability
author: "Stefano Masier, Maxime Dahirel, Frederik Mortier, Dries Bonte (this code by M. Dahirel and S. Masier)"
date:
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
editor_options:
  chunk_output_type: console
bibliography: tetranychus-metapop-2019-refs.bib
csl: journal-of-animal-ecology.csl
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE)
```


# A brief introduction

(see manuscript/preprint text for more context)

This script analyses data resulting from the study of experimental metapopulations of spider mites *Tetranychus urticae*. Each replicate metapopulation was composed of 9 patches connected by bridges; the bridge length varied between replicates. Abundances of adult females were tracked weekly in each patch of each metapopulation. We are interested in the effect of bridge length and randomization treatment (see text) on (meta)population size and various variability metrics. 

This script relies heavily on de Villemereuil *et al.* [-@villemereuilGeneralMethodsEvolutionary2016] (for converting latent means and variance-covariance matrices to data/observed scale) and Wang and Loreau [@wangEcosystemStabilitySpace2014](for the principle and formula of $\alpha$, $\beta$, $\gamma$ variability in metapopulations). Everything below assumes the reader has read these two papers.

# Part 1 : Preparation

## 1A - packages

First, we’re going to load all the packages we need. This includes the `tidyverse` packages [@wickhamWelcomeTidyverse2019], for data wrangling at the beginning and summarising/plotting at the end, as well as the packages needed to do Bayesian inference and post-processing of Bayesian model results.
This script uses the Stan language [@carpenterStanProbabilisticProgramming2017; @standevelopmentteamRStanInterfaceStan2018] for model fitting (we can use the `rstan` or the `cmdstanr` implementation; see https://mc-stan.org/ for how to install them). The `brms` package [@burknerBrmsPackageBayesian2017] allows you to write a wide range of Stan models using R syntax; it then does the “translation” before fitting. The `QGglmm` package by de Villemereuil *et al.* [-@villemereuilGeneralMethodsEvolutionary2016] is useful to convert latent scale variance-covariance matrices, like a (G)LMM output, to observed scale matrices, needed for our analysis.

```{r packages-loading}
library(rstan) ## Stan backend
#library(cmdstanr) ## Stan backend (another)
library(brms) ## the interface we are using

library(tidyverse)
library(bayesplot)
library(tidybayes)
library(matrixStats)

library(QGglmm) ## this package is needed to help convert variance-covariance matrices from latent to data scale

library(patchwork)

library(here)

## some useful default settings
rstan_options(auto_write = TRUE) #for rstan
options(mc.cores = 2) ## reduce/increase depending on cores available
N_chains <- 4
N_warmup <- 4000   ## used for publication: 10000 ## 200 is enough for tests
N_iter <- N_warmup + 2000  ## recommended for publication-level quality: N_warmup + at least 2000  
## Nwarmup + 200 is probably good enough for tests

```
k
**Note:** The finished model object will likely be large (at least several 100s Mb) and take some time to fit (a few hours to half a day on the few laptops it has been tested on). If this causes problems, we invite you to try a model with a much smaller `N_warmup` and number of iterations post-warmup (see comments in code chunk above). It should be close enough to convergence, despite warnings, to allow you to get a general idea of the results.


## 1B – data loading and wrangling

```{r data-load}
raw_data <- read_csv(here("data","tetranychus-metapop-2019-dataset.csv"))
```

The variables in `raw_data` are as follow:

- `METAPOP_ID`: unique ID for each replicate metapopulation
- `LENGTH`: length of bridges between patches in the metapopulation (4, 8, or 16 cm)
- `SHUFFLE`: within-metapopulation reshuffling treatment. NO: control, no reshuffling, R: reshuffling
- `REPLICATE`: replicate metapopulation number within a treatment set. The `METAPOP_ID` string corresponds to "`LENGTH` _ `SHUFFLE` _ `REPLICATE`"
- `PATCH`: Patch location in the 3 by 3 metapopulation (written as "row.column")
- `WEEK`: weeks of data collection (first week coded as 1)
- `AFEMA`: number of adult females counted on patch, our measure of abundance

In the raw dataset, data are stored so that there is one column for all population sizes, and a column saying which patch it belongs to. This is a tidy way to store info, but not adequate for what we want to do. To fit our multivariate model, we need one column per patch position instead:

```{r data-reshape}

data <- raw_data %>% 
  mutate(PATCH2= paste("P", PATCH, sep = "")) %>% 
  mutate(PATCH2= str_remove(PATCH2, "[.]")) %>% 
### the two lines above make patches names (a) easier to use as column names (letter as 1st character)
### and consistent with brms standards on response variable names (best to avoid dots and underscore,
### as some functions will remove them for output names and then matching input and output becomes slightly harder)
  select(-c(PATCH)) %>%
  pivot_wider(names_from = PATCH2, values_from = AFEMA) %>%
  drop_na() %>% #3 out of ~550 rows contain NAs in at least one patch, discard
  mutate(WEEK2 = paste(WEEK, METAPOP_ID))

Npatches <- length(unique(raw_data$PATCH)) ### as a check, counting how many patches there are, should be 9
### stored as may be useful later

```

The way the dataset is now, one row = one snapshot of one of the metapopulations (i.e. its state at one time point). 
The `WEEK2` variable is useful to fit separate random effects per replicate (this is because in `brms`, random effect levels for each metapop need to have different names if you fit separate random effect matrices per metapopulation).

We finish by adding a metapopulation total column, which may be useful for some comparisons:

```{r metapop-sum}
data <- data %>% 
  mutate(METAPOPSUM = select(.,P11:P33) %>% rowSums())
```


## 1-C Functions for $\alpha$, $\beta$, $\gamma$ variability

One of the aims of this script is to estimate the $\alpha$, $\beta$ and $\gamma$ variability metrics proposed by Wang and Loreau [-@wangEcosystemStabilitySpace2014], while accounting for among metapopulation-variability. The short functions below calculate these metrics, given as input a temporal variance-covariance matrix, and a vector of mean population sizes (both must be numeric and contain the same number of patches, of course).

```{r wang-loreau}

alpha_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_L = sum(sqrt(diag(varcorr))) / sum(means)
  return(CV_L^2)
}

gamma_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_M = sqrt(sum(varcorr)) / sum(means)
  return(CV_M^2)
}

# no function for beta 1, it's simply 1/phi or alpha/gamma
# the beta 2 and the phi functions are here for completeness and checks, we don't need them
# because both betas can be defined through alpha and gamma
# beta 1: see above, beta2 = alpha - gamma

phi_wang_loreau <- function(varcorr) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
  if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
  if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    
  return(sum(varcorr) / (sum(sqrt(diag(varcorr))))^2)
}

beta2_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  sqrt_w_bar <- sum(sqrt(diag(varcorr))) / length(means)
  return((sqrt_w_bar^2 - (sum(varcorr)) / (length(means)^2)) / mean(means)^2)
}
```

# Part 2 - Fitting the model

## 2A - Model description

We fit a multivariate generalized linear mixed/multilevel model to the abundance data, with one submodel for each of the 9 patches. This Poisson model (because count data) includes "random" effects of metapopulation ID (to account for the fact some replicates may have higher/lower average population sizes). It also includes a time random effect, to account for temporal patch variance-covariance. This is this second effect we are most interested in here.

The formula for the model for the number of adult females $N_{i,x,y,t}$ in metapopulation $i$, in the patch of coordinates $x,y$ at time $t$ is

$$\begin{equation*}
N_{[i,x,y,t]} \sim  {\textrm{Poisson}}(\lambda_{[i,x,y,t]}) \\
\log(\lambda_{[i,1,1,t]}) = \alpha_{[1,1]\textrm{TREATMENT}[i]} + \beta_{[i,1,1]} + \gamma_{[i,1,1,t]} \\
...\\
\log(\lambda_{[i,3,3,t]}) = \alpha_{[3,3]\textrm{TREATMENT}[i]} + \beta_{[i,3,3]} + \gamma_{[i,3,3,t]} \\

\mathbf{}\left[\begin{matrix}
\beta_{[i,1,1]} \\ ... \\ \beta_{[i,3,3]} 
\end{matrix}\right] \sim {\textrm{MVNormal}}\left(

\mathbf{}
\left[\begin{matrix}
0 \\ ... \\ 0 
\end{matrix}\right] ,
\boldsymbol{\Omega}_{\textrm{METAPOP}}\right)\\

\left[\begin{matrix}
\gamma_{[i,1,1,t]} \\ ... \\ \gamma_{[i,3,3,t]} 
\end{matrix}\right] \sim {\textrm{MVNormal}}\left(

\left[\begin{matrix}
0 \\ ... \\ 0 
\end{matrix}\right] ,
\boldsymbol{\Omega}_{\textrm{TIME}[i]}\right)\\

\boldsymbol{\Omega}_{\textrm{METAPOP}} = 
\left(\begin{matrix}
\sigma_{M[1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{M[3,3]} 
\end{matrix}\right) 
\boldsymbol{R}_{\textrm{METAPOP}}
\left(\begin{matrix}
\sigma_{M[1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{M[3,3]} 
\end{matrix}\right)    \\

\boldsymbol{\Omega}_{\textrm{TIME}[i]} = 
\left(\begin{matrix}
\sigma_{T[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{T[i,3,3]} 
\end{matrix}\right)  
\boldsymbol{R}_{\textrm{TIME}[i]}
\left(\begin{matrix}
\sigma_{T[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{T[i,3,3]} 
\end{matrix}\right)    \\

\end{equation*}$$

where $\alpha$ are treatment-specific latent patch intercepts, $\beta$ metapopulation-specific deviations from these intercepts, $\gamma$ temporal abundance fluctuations (not to be confused with $\alpha$, $\beta$, $\gamma$ variabilities), $\boldsymbol{\Omega}$ the relevant covariance matrices and $\boldsymbol{R}$ the correlation matrices.

You can see we are estimating a separate temporal variance-covariance matrix for each replicate $i$. We will average everything downstream as needed, but given the non-linearities everywhere in a GLMM, it is probably best for the variabilities to calculate everything replicate by replicate first and only average later (we'll come back to these non-linearities later). It is important to note that it means this is a model with observation-level random effects [@harrisonUsingObservationlevelRandom2014], so no further overdispersion to worry about.

Similarly, we can write a (simpler) model for the total metapopulation size $M$ (the `METAPOPSUM` column):

$$
\begin{equation*}
M_{[i,t]} \sim  {\textrm{Poisson}}(\lambda_{[i,t]}) \\
\log(\lambda_{[i,t]}) = \alpha_{\textrm{TREATMENT}[i]} + \beta_{[i]} + \gamma_{[i,t]} \\

\beta_{[i]} \sim \mathrm{Normal}(0, \sigma_{\beta}) \\
\gamma_{[i,t]} \sim \mathrm{Normal}(0, \sigma_{\gamma[i]})

\end{equation*}
$$
(the temporal variance is again replicate-specific and serves again as an observation-level random effect)

## 2B - Implementation - formulas

This is implemented in R as follow. First, we need to create a `TREATMENT` variable combining `LENGTH` and `SHUFFLE`. We're also going to create some duplicate columns, just so that the random effects from the patch-by-patch model and from the "all-metapop" model don't end up in the same slots of the fitted model object (it complicates postprocessing otherwise):

```{r treatment-create}
data <- data %>% 
  mutate(TREATMENT = interaction(LENGTH, SHUFFLE)) %>% 
  mutate(WEEK3 = WEEK2) %>% 
  mutate(METAPOP_ID2 = METAPOP_ID)
```

Then we build the formula for both the patch by patch multivariate model and the metapopulation total univariate model:

```{r formula}
## see vignette("brms_multivariate") for help on the syntax, in particular the (1|P|random effect) syntax
## see ?gr() for help too if needed
metapopformula <- mvbf(
      bf(P11 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P12 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P13 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P21 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P22 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P23 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P31 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P32 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      bf(P33 ~ 0 + TREATMENT + (1 | P | METAPOP_ID) + (1 | Q | gr(WEEK2, by = METAPOP_ID)), family = poisson),
      
      bf(METAPOPSUM ~ 0 + TREATMENT + (1|METAPOP_ID2) + (1|gr(WEEK3, by = METAPOP_ID2)), family = poisson)
    )
```

note the `~0 + TREATMENT`, rather than `~TREATMENT` syntax in the fixed effects. It’s just a writing trick to directly obtain the coefficients we want and reduce the quantity of post-processing calculations needed, and be sure all prior means have the same precision. See Schielzeth [-@schielzethSimpleMeansImprove2010] and McElreath [-@mcelreathStatisticalRethinkingBayesian2020].  

see also the fact we "combined" the multivariate patch-level model and the univariate metapopulation-level model in one object (they aren't connected though, see the random effects). It's just to fit them in the same `brm` call to make it easier to compare the posteriors later if we want.

## 2C - Implementation - choosing priors

We then select our priors. The priors we propose here are weakly informative [sensu @mcelreathStatisticalRethinkingBayesian2020]. For the fixed effects coefficients (corresponding to latent, i.e. on log scale, grand means of single patch population size), I based myself roughly on the entire distribution of non-zero population densities in De Roissart *et al.* [-@deroissartDataSpatialSpatiotemporal2016; -@deroissartSpatialSpatiotemporalVariation2015] (multiplied to account for our patch size: 5 by 5 cm). Note that the prior we use is slightly wider than what could be done based on density distribution in De Roissart *et al.* (which would be something like $\textrm{Normal}(1.8,0.9)$ for one patch), mostly to have round numbers, but also to allow for a slightly wider prior.  We used a $\textrm{Normal}(4.2,1)$ for the corresponding parameter for the metapopulation-level model; this correspond roughly to a multiplication by 9 of the prior mean compared to the prior chosen for the patch-level approach, and there are 9 patches in each metapopulation.

```{r prior-setting}
  prior <- c(
    ### PRIORS FOR THE PATCH-LEVEL MODEL
    set_prior("normal(2,1)", class = "b", 
              resp = c("P11", "P12", "P13", "P21", "P22", "P23", "P31", "P32", "P33")),
    set_prior("normal(0,1)", class = "sd", group = "METAPOP_ID", 
              resp = c("P11", "P12", "P13", "P21", "P22", "P23", "P31", "P32", "P33")),
    set_prior("normal(0,1)", class = "sd", group="WEEK2", 
              resp = c("P11", "P12", "P13", "P21", "P22", "P23", "P31", "P32", "P33")),
    set_prior("lkj(2)", class = "cor"), # mildly skeptical of very high correlations (positive or negative)
    ### PRIORS FOR THE METAPOP LEVEL MODEL
    set_prior("normal(4.2,1)", class = "b", resp = "METAPOPSUM"),
    set_prior("normal(0,1)", class = "sd", group = "METAPOP_ID2", resp = "METAPOPSUM"),
    set_prior("normal(0,1)", class = "sd", group = "WEEK3", resp = "METAPOPSUM")
  )
```

## 2D - Implementation - fitting

We can now fit the model based on the choices we made earlier

```{r model_fitting}
if(file.exists(here("R_output","model.Rdata"))){
  # this if-else statement is avoid re-fitting a model if there is already one existing in R_output
  # to override, re-run the model and re-save manually by selecting only the relevant code lines (or delete the Rdata object before relaunching this code chunk)
  load(here("R_output","model.Rdata"))
  }else
    {

mod <- brm(metapopformula, data = data, 
           iter = N_iter, warmup = N_warmup, chains = N_chains, 
           prior = prior, 
           seed = 42,
           control = list(adapt_delta = 0.8), #default; adapt_delta and other controls can be adjusted if needed
           backend= "rstan")

##with rstan as backend, the model *may* throw some some rhats=NA warnings on exit
## they can (for once) be safely ignored: related issue: https://github.com/paul-buerkner/brms/issues/865
## these warnings don't pop up when cmdstanr is used as backend 
## (other "normal" warnings will appear if needed (e.g during a test run with low iters) with both backends)
## or when doing summary(mod)
## you can check that only parameters that should be constant give NA warnings through rhat(mod)
## and quickly look at the rhats:
## hist(rhat(mod));abline(v=1.01,col="red")
save(list="mod", file=here("R_output","model.Rdata"))
}
```

Note that it can take some time to finish (around 2000-4000 iterations/h/core on my laptop).

```{r model_summary}
summary(mod)  #look at Rhat and ESS both tail and bulk
```

You also need to check that the model can reproduce the data well, through posterior predictive checks [@gabryVisualizationBayesianWorkflow2019]
(here, given we have observation-level random effects, it should be OK)

```{r model_ppchecks}

pp_check(mod, resp = "P11", nsamples = 20)
## can the model generate the observed data? (there are many ppcheck possibles, see the help of the bayesplot package)
## in a multivariate model, you need to specify the response (so here which patch)
```

# Part 3 - Extracting key metrics from model output

Once, we have fitted and chosen a model, it's time to extract what we need from it. In the next code chunk, I've made a function that takes a `brms` model as input and give all metrics we need as output (it only works correctly on one of the models that can be specified via the script above, of course).

The function is *far* from optimized in terms of time or memory use (and I would probably have a look if I had time, e.g. move from `for` loops to `purr`), but it works so <shrug>.

In some places in the function you will see 3-dimensional arrays called. This is going to be the typical structure of many parts of a multivariate model fitted by `brms`:

model_part[X,Y,Z],
X = posterior sample index
Y = data observation row
Z = response variable (here, patches)

```{r metrics_calculation_function}
### slow !!!!! need a rewrite to optimise
metrics_estimate <- function(mod = mod) {
  data_results <- list(NA)

  METAPOPlevels <- levels(factor(data$METAPOP_ID))
  ###an object with each metapop name so we can loop and call their names as needed

  for (h in 1:length(METAPOPlevels)) { ## we do everything metapop by metapop first

    data_subset <- subset(data, METAPOP_ID == METAPOPlevels[h])

    patch_by_patch_Nmean <- apply(fitted(mod, newdata = data_subset, summary = FALSE), 3, rowMeans)
    ## predictions of the number of mites for each patch at each time

    metapopNmean <- rowSums(patch_by_patch_Nmean) # metapop size = sum of all patches
    patchNmean <- metapopNmean / 9

    ### there are three types of patch in a metapop differing in level of local connectedness
    ### we can calculate mean patch size for each of them and compare them
    Nmean_corners <- rowMeans(patch_by_patch_Nmean[, c(1, 3, 7, 9)])
    Nmean_center <- patch_by_patch_Nmean[, 5]
    Nmean_sides <- rowMeans(patch_by_patch_Nmean[, c(2, 4, 6, 8)])

    ### get intercept means on the latent scale, excluding time effect
    ### BUT including the METAPOP_ID average effect
    ### (remember, we are working replicate by replicate, so this is needed
    ### to have the correct latent intercept)
    if (sum(is.na(which.trt))> 0) {
      latent_intercept <- as.data.frame(fixef(mod, summary = FALSE)) +
        ranef(mod, summary = FALSE)$METAPOP_ID[, METAPOPlevels[h], ]
    }

    if (sum(is.na(which.trt))== 0) {
      latent_intercept <- as.data.frame(fixef(mod, summary = FALSE)) %>%
        select(contains(paste("TREATMENT", unique(data_subset$TREATMENT), sep = ""))) +
        ranef(mod, summary = FALSE)$METAPOP_ID[, METAPOPlevels[h], ]
    }


    ### get the latent TIME variance-covariance matrix for the metapop we're currently in
    include <- colnames(VarCorr(mod, summary = FALSE)$WEEK2$cov)
    include <- include[str_detect(include, pattern = METAPOPlevels[h])]
    vcv_latent_time <- VarCorr(mod, summary = FALSE)$WEEK2$cov[, include, include]


    ### convert from latent TIME VCV matrix to observed scale VCV matrix
    ## first, create a dummy array; values will be filled with correct values below
    vcv_obs_time <- vcv_latent_time

    ## !Read the de Villemereuil et al Genetics paper and QGglmm package help for details
    ## but broad description of what happens below is
    ## for each posterior sample (nsamples(mod)), create a vector psi, then fill it with value for each patch
    ## then use this psi to get the observed VCV from the latent VCV
    ## repeat for all posterior samples
    for (i in 1:nsamples(mod)) {
      psiM <- NA
      for (j in 1:9) {
        psiM[j] <- QGpsi(latent_intercept[i, j], vcv_latent_time[i, j, j], d.link.inv = function(x) {
          exp(x)
        })
      }
      psiM <- diag(psiM)
      vcv_obs_time[i, , ] <- psiM %*% vcv_latent_time[i, , ] %*% t(psiM)
    }


    ### calculate our posterior variability metrics
    alpha <- NA
    gamma <- NA
    for (i in 1:nsamples(mod)) {
      alpha[i] <- alpha_wang_loreau(vcv_obs_time[i, , ], patch_by_patch_Nmean[i, ])
      gamma[i] <- gamma_wang_loreau(vcv_obs_time[i, , ], patch_by_patch_Nmean[i, ])
    }

    ### group our metrics of interest in a data.frame
    metrics <- data.frame(
      metapopNmean = metapopNmean, patchNmean = patchNmean,
      Nmean_corners = Nmean_corners, Nmean_center = Nmean_center, Nmean_sides = Nmean_sides,
      alpha = alpha, beta1 = alpha / gamma, beta2 = alpha - gamma, gamma = gamma,
      METAPOP_ID = as.character(METAPOPlevels[h]), TREATMENT = "all", which_model = deparse(substitute(mod))
    ) %>%
      ### precalculate within metapopulations differences BEFORE averaging
      mutate(
        deltaN_corners_center = Nmean_corners - Nmean_center,
        deltaN_sides_center = Nmean_sides - Nmean_center,
        deltaN_corners_sides = Nmean_corners - Nmean_sides
      )

    ### if the model has a TREATMENT effect, replace the default ("all") by the correct TREATMENT in the output table
    if (sum(is.na(which.trt)) == 0) {
      metrics$TREATMENT <- as.character(unique(data_subset$TREATMENT))
    }

    ### add MCMC iteration number to table, important to compute differences among groups later
    metrics <- as_tibble(metrics) %>%
      add_column(.iteration = 1:dim(metrics)[1]) ## across all chains together

    ### we store the output for each metapop in a list...
    data_results[[h]] <- metrics
  }

  ### ...and then we bind all the tables in the list together
  data_results <- bind_rows(data_results) %>% #I need to find a way not to have the binding character and factor warning (it's just a warning)
    group_by(TREATMENT, which_model, .iteration) %>%
    summarize_if(is.numeric, mean) %>%
    ungroup()
  ## the very last line above is where we finally group metrics calculated at the replicate level
  ## and average them through the whole dataset/by treatment

  return(data_results)
}
```

Once the function is in the environment, to get what we want, we "just" have to do this:
```{r get_results}

if(file.exists(here("R_output","tab_results.Rdata")))
  {
  load(here("R_output","tab_results.Rdata"))
  }else
    {

tab_results <- metrics_estimate(mod = mod) ## change model name as appropriate here

save(list="tab_results", file=here("R_output","tab_results.Rdata"))
}
```


# Part 4- Figures

Before doing the figures, we need to recreate the `SHUFFLE` and `LENGTH` columns. We also need to create color palettes that'll be used throughout:

```{r figure_prep}
tab_results <-tab_results %>% 
  mutate(
    SHUFFLE=str_extract(TREATMENT,"[:upper:]+"),
    LENGTH=str_extract(TREATMENT,"[:digit:]+") %>% as.numeric()
         )

paletteLENGTH <- c("#D55E00", "#E69F00", "#F0E442") #for figures 2 to 4
paletteLOCAL <-  c("#e66101", "#fdb863", "#5e3c99") #for figure 5
```

## Figure 2

Figs 2 to 4 are each made of 4 panels, one per key variable (metapopulation size, $\alpha,\beta,\gamma$ variabilities). We create each panel separately then use the `patchwork` package to merge them and to apply styles common to all panels.

Figure 2 contains posteriors for the non-shuffled (control) metapopulations:

```{r fig2}

tab2 <- tab_results %>% 
  filter(SHUFFLE == "NO")

p2a <- tab2 %>% 
  ggplot()+
  stat_halfeye(aes(x=metapopNmean,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  labs(x = "Metapopulation mean density")+
  scale_y_discrete("Length (cm)")
  
p2b <- tab2 %>% 
  ggplot()+
  stat_halfeye(data= . %>% filter(alpha<=20), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=alpha,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=alpha,y=factor(LENGTH)),
               point_interval = median_hdi)+
  scale_x_continuous("Alpha variability")+
  scale_y_discrete("")

p2c <- tab2 %>% 
  ggplot()+
  stat_halfeye(aes(x=beta1,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy")+
  scale_x_continuous("Beta variability")+
  scale_y_discrete("Length (cm)")

p2d <- tab2 %>% 
  ggplot()+
  stat_halfeye(data=. %>% filter(gamma<=15), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=gamma,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=gamma,y=factor(LENGTH)),
               point_interval = median_hdi)+
  scale_x_continuous("Gamma variability") +
  scale_y_discrete("")

(p2a | p2b)/ (p2c | p2d) + plot_annotation(tag_levels = 'A') & 
  scale_fill_manual(values = paletteLENGTH) &
  theme_bw() & 
  theme(legend.position = "none", text=element_text(size = 20))
```

## Figure 3

Figure 3 contains posteriors for the shuffled metapopulations:

```{r fig3}

tab3 <- tab_results %>% 
  filter(SHUFFLE == "R")

p3a <- tab3 %>% 
  ggplot()+
  stat_halfeye(aes(x=metapopNmean,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  scale_x_continuous("Metapopulation mean density")+
  scale_y_discrete("Length (cm)")
  
p3b <- tab3 %>% 
 ggplot()+
  stat_halfeye(data=. %>% filter(alpha<=20), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=alpha,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=alpha,y=factor(LENGTH)),
               point_interval = median_hdi)+
  scale_x_continuous("Alpha variability")+
  scale_y_discrete("")

p3c <- tab3 %>% 
  ggplot()+
  stat_halfeye(aes(x=beta1,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy")+
  scale_x_continuous("Beta variability")+
  scale_y_discrete("Length (cm)")

p3d <- tab3 %>% 
  ggplot()+
  stat_halfeye(data= . %>% filter(gamma<=15), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=gamma,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=gamma,y=factor(LENGTH)),
               point_interval = median_hdi)+
  scale_x_continuous("Gamma variability") +
  scale_y_discrete("")

(p3a | p3b)/ (p3c | p3d) + plot_annotation(tag_levels = 'A') & 
  scale_fill_manual(values = paletteLENGTH) & 
  theme_bw() & 
  theme(legend.position = "none", text=element_text(size = 20))

```

## Figure 4

Figure 4 contains posteriors of the comparison between shuffled and non shuffled treatments. There are two options here, either use the difference between treatments (chunk `fig4`), or the ratio (`4bis`) between treatments. The ratio may be more appropriate since we are dealing with multiplicative processes:

### Figure 4: additive comparison

```{r fig4}

p4a <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "metapopNmean", by = SHUFFLE) %>%
  ggplot() +
  stat_halfeye(aes(y = factor(LENGTH), x = metapopNmean, fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  geom_vline(xintercept = 0) + 
  scale_x_continuous("Metapopulation mean density") +
  scale_y_discrete("Length (cm)")

p4b <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "alpha", by = SHUFFLE) %>%
  ggplot() +
  stat_halfeye(data=. %>% filter(abs(alpha)<=15), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=alpha,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=alpha,y=factor(LENGTH)),
               point_interval = median_hdi)+
  geom_vline(xintercept = 0) + 
  scale_x_continuous("Alpha variability") +
  scale_y_discrete("")

p4c <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "beta1", by = SHUFFLE) %>%
  ggplot() +
  stat_halfeye(aes(y = factor(LENGTH), x = beta1,fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  geom_vline(xintercept = 0) +
  scale_x_continuous("Beta variability") +
  scale_y_discrete("Length (cm)")

p4d <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "gamma", by = SHUFFLE) %>%
  ggplot() +
  stat_halfeye(data=. %>% filter(abs(gamma)<=5), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=gamma,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=gamma,y=factor(LENGTH)),
               point_interval = median_hdi)+
  geom_vline(xintercept = 0) +
  scale_x_continuous("Gamma variability") +
  scale_y_discrete("")


(p4a | p4b)/ (p4c | p4d) + plot_annotation(title = "Comparisons, shuffled - control", tag_levels = 'A') & 
  theme_bw() & 
  scale_fill_manual(values = paletteLENGTH) & 
  theme(legend.position = "none", text=element_text(size = 20))
```

### Figure 4bis: multiplicative comparison

```{r fig4bis}
p4a <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "metapopNmean", by = SHUFFLE, fun= `/`) %>%
  ggplot() +
  stat_halfeye(aes(y = factor(LENGTH), x = metapopNmean, fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  geom_vline(xintercept = 1) + 
  scale_x_continuous("Metapopulation mean density") +
  scale_y_discrete("Length (cm)")

p4b <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "alpha", by = SHUFFLE, fun= `/`) %>%
  ggplot() +
  stat_halfeye(data=. %>% filter(alpha<=10), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=alpha,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=alpha,y=factor(LENGTH)),
               point_interval = median_hdi)+
  geom_vline(xintercept = 1) + 
  scale_x_continuous("Alpha variability") +
  scale_y_discrete("")

p4c <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "beta1", by = SHUFFLE, fun= `/`) %>%
  ggplot() +
  stat_halfeye(aes(y = factor(LENGTH), x = beta1,fill=factor(LENGTH)),
               point_interval = median_hdi,normalize="xy") +
  geom_vline(xintercept = 1) +
  scale_x_continuous("Beta variability") +
  scale_y_discrete("Length (cm)")

p4d <- tab_results %>% 
  group_by(LENGTH) %>% 
  compare_levels(variable = "gamma", by = SHUFFLE, fun= `/`) %>%
  ggplot() +
  stat_halfeye(data=. %>% filter(gamma<=5), ##artificial adjustment for plotting, remove beyond 99% quantile for readability
               aes(x=gamma,y=factor(LENGTH),fill=factor(LENGTH)),
               point_interval = NULL,normalize="xy") +
  stat_pointinterval(aes(x=gamma,y=factor(LENGTH)),
               point_interval = median_hdi)+
  geom_vline(xintercept = 1) +
  scale_x_continuous("Gamma variability") +
  scale_y_discrete("")


(p4a | p4b)/ (p4c | p4d) + plot_annotation(title = "Comparisons, ratio shuffled / control", tag_levels = 'A') & 
  theme_bw() & 
  scale_fill_manual(values = paletteLENGTH) & 
  theme(legend.position = "none", text=element_text(size = 20))
```

## Figure 5

Figure 5 shows what happens at the local patch scale, depending on the local level of patch connectedness:

```{r fig5}
tab_results %>% 
  select(Nmean_corners,Nmean_center,Nmean_sides, LENGTH, SHUFFLE,.iteration) %>%
  pivot_longer(cols=c(Nmean_corners,Nmean_center,Nmean_sides),
               values_to="N_local") %>% 
  mutate(name = fct_recode(factor(name), corner="Nmean_corners",edge="Nmean_sides",center="Nmean_center")) %>% 
  mutate(`local connectedness` = fct_relevel(name,"corner",after=Inf)) %>% 
  mutate(SHUFFLE = fct_recode(factor(SHUFFLE),control="NO",randomized="R")) %>% 
  ggplot()+
  stat_halfeye(aes(x=N_local,y=factor(LENGTH),fill=`local connectedness`, alpha=0.5))+
  scale_x_continuous("mean local population density")+
  scale_fill_manual(values=paletteLOCAL)+
  scale_y_discrete("Length (cm)")+
  scale_alpha(guide="none")+
  facet_grid(rows=vars(SHUFFLE))+
  theme_bw() & 
  theme(legend.position = "bottom", text=element_text(size = 20))
  
```

# More on inferences

The function `compare_levels()` used to make Figure 4 can be used to obtain actual numbers and intervals for many comparisons. Only one example displayed here for simplicity, see the help of both functions for more details, and explore:

```{r compare}
## an example
## calculating differences between treatment using compare_levels
compare_levels(tab_results, variable = "metapopNmean", by = TREATMENT) %>%
  mean_hdi()


```

# References




