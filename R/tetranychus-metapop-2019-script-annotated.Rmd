---
title: Analysis of replicated *Tetranychus* metapopulation experiment - abundance and
  temporal variability
author: "Stefano Masier, Maxime Dahirel, Frederik Mortier, Dries Bonte (this code by M. Dahirel and S. Masier)"
date:
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
editor_options:
  chunk_output_type: console
bibliography: tetranychus-metapop-2019-refs.bib
csl: journal-of-animal-ecology.csl
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE)
```


# A brief introduction

(see manuscript/preprint text for more context)

This script analyses data resulting from the study of experimental metapopulations of spider mites *Tetranychus urticae*. Each replicate metapopulation was composed of 9 patches connected by bridges; the bridge length varied between replicates. Abundances of adult females were tracked weekly in each patch of each metapopulation. We are interested in the effect of bridge length and randomization treatment (see text) on mean patch-level population size, mean metapopulation size and various variability metrics. 

This script relies heavily on de @villemereuilGeneralMethodsEvolutionary2016 (for converting latent means and variance-covariance matrices to data/observed scale) and @wangEcosystemStabilitySpace2014 (for the principle and formulas of $\alpha$, $\beta$, $\gamma$ variability in metapopulations). Everything below assumes the reader has read these two papers.

# Part 1 : Preparation

## 1A - packages

First, we’re going to load all the packages we need. This includes the `tidyverse` packages [@wickhamWelcomeTidyverse2019], for data wrangling at the beginning and summarising/plotting at the end, as well as the packages needed to do Bayesian inference and post-processing of Bayesian model results.
This script uses the Stan language [@carpenterStanProbabilisticProgramming2017] for model fitting (we can use the `rstan` or the `cmdstanr` implementation; see https://mc-stan.org/ for how to install them). The `brms` package [@burknerBrmsPackageBayesian2017] allows you to write a wide range of Stan models using R syntax; it then does the “translation” itself before fitting. The `QGglmm` package by de Villemereuil *et al.* [-@villemereuilGeneralMethodsEvolutionary2016] is useful to convert latent scale variance-covariance matrices from a GLMM output to observed scale matrices, needed for our analysis.

```{r packages-loading}
#library(rstan) ## Stan backend

library(cmdstanr)    # [github::stan-dev/cmdstanr] v0.3.0 
## Stan backend (another)

library(brms)        # CRAN v2.14.4 
## the interface we are using


library(tidyverse)   # CRAN v1.3.0
library(bayesplot)   # CRAN v1.8.0
library(tidybayes)   # CRAN v2.3.1
library(matrixStats) # CRAN v0.58.0

library(QGglmm)      # CRAN v0.7.4 
## this package is needed to help convert variance-covariance matrices from latent to data scale

library(patchwork)   # [github::thomasp85/patchwork] v1.1.0.9000 
#plotting

library(here)        # CRAN v1.0.1

## some useful default settings
# rstan_options(auto_write = TRUE) #for rstan
options(mc.cores = 4) ## reduce/increase depending on cores available
N_chains <- 4
N_warmup <- 2000   ## used for publication: 2000 ## 200 is enough for tests
N_iter <- N_warmup + 2000  ## recommended for publication-level quality: 2000 iterations post warmup when all chains combined 
## Nwarmup + 200 is probably good enough for tests

```

**Note:** The finished model object for the patch level will likely be large (several 100s of MB, depending on `N_iter`) and take some time to fit (a few hours to half a day on the few laptops it has been tested on). If this causes problems, we invite you to try a model with a much smaller `N_warmup` and number of iterations post-warmup `N_iter` (see comments in code chunk above). It should be close enough to convergence, despite warnings, to allow you to get a general idea of the results.


## 1B – data loading and wrangling

```{r data-load}
raw_data <- read_csv(here("data","tetranychus-metapop-2019-dataset.csv"))
```

The variables in `raw_data` are as follow:

- `METAPOP_ID`: unique ID for each replicate metapopulation
- `LENGTH`: length of bridges between patches in the metapopulation (4, 8, or 16 cm)
- `SHUFFLE`: within-metapopulation reshuffling treatment. NO: control, no reshuffling, R: reshuffling
- `REPLICATE`: replicate metapopulation number within a treatment set. The `METAPOP_ID` string corresponds to "`LENGTH`_`SHUFFLE`_`REPLICATE`"
- `PATCH`: Patch location in the 3 by 3 metapopulation (written as "row.column"). Importantly for the analysis, note that saying which axis of a replicate is the row and which is the column is 100% arbitrary
- `WEEK`: weeks of data collection (first week coded as 1)
- `AFEMA`: number of adult females counted on patch, our measure of abundance

In the raw dataset, data are stored in the long format (one row = one patch observation at one moment of time). We're going to temporarily switch data to the wide format (one row = snapshot of an entire replicate at one moment in time, with one column by patch), so that it's easier to do a few things:

- a few replicates have NAs on *some* patches on one week or so, for some reason (transcription error?). To be safe, let's remove the entire week for the metapopulation in question, even if most patches were recorded.

- we're going to need a variable representing the total metapopulation size for each replicate and week, and it's just easier to do it from the wide table

```{r data-reshape}

data_wide <- raw_data %>% 
  mutate(PATCH= paste("P", PATCH, sep = "")) %>% 
  mutate(PATCH= str_remove(PATCH, "[.]")) %>% 
### the two lines above make patches names easier to use as column names (letter as 1st character)
  mutate(LENGTH = fct_recode(factor(LENGTH),
                             `16 cm (low)` = "16",
                             `8 cm (medium)` = "8",
                             `4 cm (high)` = "4"),
         SHUFFLE = fct_recode(as.factor(SHUFFLE), 
                              `control`="NO",
                              `randomized`="R")) %>% 
  pivot_wider(names_from = PATCH, values_from = AFEMA) %>%
  drop_na() #3 out of ~550 rows contain NAs in at least one patch, discard

```

From here, we create two tables. One for the patch-by-patch model, where we switch back to the long format. One for a model where metapopulation size is the response, where we keep the table in its current format and just create a sum column.

```{r data-patch}
P_data <- data_wide %>% 
  pivot_longer(P11:P33, names_to = "PATCH", values_to = "AFEMA") %>% 
  mutate(local_connectedness = 1 + 1 * (PATCH %in% c("P12","P21","P23","P32"))+ 
                               2 * (PATCH %in% c("P11","P13","P31","P33"))) %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "1", 
                                          `side (medium)` = "2", 
                                          `corner (low)`="3")) 

## adding dummy centered variables
P_data <- P_data %>% 
  mutate(is.randomized = as.numeric(SHUFFLE =="randomized") - mean(as.numeric(SHUFFLE =="randomized"))) %>% 
  mutate(is.local5 = as.numeric(local_connectedness =="side (medium)") - mean(as.numeric(local_connectedness =="side (medium)")),
         is.local3 = as.numeric(local_connectedness =="corner (low)") - mean(as.numeric(local_connectedness =="corner (low)")),
         is.landscape8 = as.numeric(LENGTH =="8 cm (medium)") - mean(as.numeric(LENGTH =="8 cm (medium)")),
         is.landscape16 = as.numeric(LENGTH =="16 cm (low)") - mean(as.numeric(LENGTH =="16 cm (low)")))
```


```{r data-metapop}
M_data <- data_wide %>% 
  mutate(METAPOPSUM = select(.,P11:P33) %>% rowSums()) %>% 
  select(METAPOP_ID,LENGTH,SHUFFLE,REPLICATE,WEEK,METAPOPSUM)%>% 
  mutate(LENGTH = factor(LENGTH))

## adding dummy centered variables
M_data <- M_data %>% 
  mutate(is.randomized = as.numeric(SHUFFLE =="randomized") - mean(as.numeric(SHUFFLE =="randomized"))) %>% 
  mutate(is.landscape8 = as.numeric(LENGTH =="8 cm (medium)") - mean(as.numeric(LENGTH =="8 cm (medium)")),
         is.landscape16 = as.numeric(LENGTH =="16 cm (low)") - mean(as.numeric(LENGTH =="16 cm (low)")))

```

(from here onwards, anything prefixed by `P_` refers to the patch-level analysis, anything prefixed by `M_` to the metapopulation-level analysis. Some things will be prefaced by `P_M_`; these will be metapop-level things estimated from the patch-level model).

## 1-C Functions for $\alpha$, $\beta$, $\gamma$ variability

One of the aims of this script is to estimate the $\alpha$, $\beta$ and $\gamma$ variability metrics proposed by Wang and Loreau [-@wangEcosystemStabilitySpace2014], while accounting for both uncertainty and among-metapopulation variation. The short functions below calculate these metrics, given as input a temporal variance-covariance matrix, and a vector of mean patch population sizes (both must be numeric and contain the same number of patches, of course)(and also both should be on the *observed/data* scale in the case of a GLMM).

```{r wang-loreau}

alpha_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_L = sum(sqrt(diag(varcorr))) / sum(means)
  return(CV_L^2)
}

gamma_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  CV_M = sqrt(sum(varcorr)) / sum(means)
  return(CV_M^2)
}

# no function for beta 1, it's simply 1/phi or alpha/gamma
# the beta 2 and the phi functions are here for completeness and checks, we don't need them
# because both betas can be defined through alpha and gamma
# beta 1: see above, beta2 = alpha - gamma

phi_wang_loreau <- function(varcorr) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
  if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
  if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    
  return(sum(varcorr) / (sum(sqrt(diag(varcorr))))^2)
}

beta2_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  sqrt_w_bar <- sum(sqrt(diag(varcorr))) / length(means)
  return((sqrt_w_bar^2 - (sum(varcorr)) / (length(means)^2)) / mean(means)^2)
}

uneven_wang_loreau <- function(varcorr, means) {
  varcorr= as.matrix(varcorr) #ensure there is no problems with diag(varcorr) if varcorr is a 1*1 matrix
  #Basic error checks
    if(!is.vector(means)) stop("Error(not a vector): means should be a vector of numeric values (abundances)")
    if(!is.numeric(means)) stop("Error(not numeric): means should be a vector of numeric values (abundances)")
    if(length(dim(varcorr))>2) stop("Error: varcorr matrix should be a 2-dimensional matrix")
    if(dim(varcorr)[1] != dim(varcorr)[2]) stop("Error: varcorr matrix should be a square matrix")
    if(dim(varcorr)[1] != length(means)) stop("Error: varcorr matrix and means should have same dimensions")
  
  sqrt_w_bar <- sum(sqrt(diag(varcorr))) / length(means)
  
  a <- (means - mean(means))^2
  b <- (sqrt(diag(varcorr)) - sqrt_w_bar)^2
  c <- (length(means) * mean(means)^2)
  return(sum(a + b)/c)
}


```

# Part 2 - Fitting the model

## 2A - Models description

### Patch-level model

We fit a generalized linear mixed/multilevel model to the abundance data. This Poisson model (because count data) includes "random" effects of metapopulation ID (to account for the fact some replicates may have higher/lower average population sizes) and patch nested in ID (because patches may differ beyond the effect of treatment, local connectedness _ both included as fixed effects _ and replicate). Importantly, it also includes a time random effect, to account for temporal patch variance-covariance.
You can see below that we are estimating a separate temporal variance-covariance matrix for each replicate $i$. This means that (i) each patch has its own temporal variance, and patches form the same replicate can be correlated. We will average everything downstream as needed, but given the non-linearities everywhere in a GLMM, and the fact patch names are arbitrary (we could have rotated metapopulations without changing anything meaningful), it is probably best for the variabilities to calculate everything replicate by replicate first and only average later. It is important to note that the presence of the temporal matrices it means this is a model with observation-level random effects [@harrisonUsingObservationlevelRandom2014], so no further overdispersion to worry about.

Intuition behind this formulation (latent residuals via OLRE + specified covariance structure) is the same as in this comment by Paul Bürkner (https://github.com/paul-buerkner/brms/issues/600#issuecomment-511677732) The formula for the model for the number of adult females $N_{i,x,y,t}$ in metapopulation $i$, in the patch of coordinates $x,y$ at time $t$ is

$$
N_{[i,x,y,t]} \sim  {\textrm{Poisson}}(\lambda_{[i,x,y,t]}),
$$
$$
\log(\lambda_{[i,x,y,t]}) = \beta_{0} + \sum_{j=1}^J({\beta_{j} \times x_{j[i,x,y]}}) + \alpha_{[i]} + \gamma_{[i,x,y]} + \eta_{[i,x,y,t]},
$$
$$
\alpha_{[i]} \sim \mathrm{Normal}(0, \sigma_{\alpha}),
$$
$$
\gamma_{[i,x,y]} \sim \mathrm{Normal}(0, \sigma_{\eta}),
$$
$$
\begin{bmatrix} \eta_{[i,1,1,t]} \\ ... \\ \eta_{[i,3,3,t]} \end{bmatrix} 
\sim 
\textrm{MVNormal}
\begin{pmatrix}
\begin{bmatrix} 0 \\ ... \\ 0  \end{bmatrix},
\boldsymbol{\Omega}_{[i]}
\end{pmatrix},
$$
$$
\boldsymbol{\Omega}_{[i]} = 
\begin{bmatrix}
\sigma_{\eta[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{\eta[i,3,3]} 
\end{bmatrix}
\boldsymbol{R}_{[i]}
\begin{bmatrix}
\sigma_{\eta[i,1,1]} & 0 & ... \\ 0 & \ddots  \\ \vdots & & \sigma_{\eta[i,3,3]} 
\end{bmatrix},
$$

where $\beta$ are the fixed effects (with $\beta_{0}$ the intercept), $\alpha$ are replicate/metapopulation random effects, $\gamma$ patch-level random effects, and $\eta$ temporal abundance fluctuations (not to be confused with $\alpha$, $\beta$, $\gamma$ variabilities). $\boldsymbol{\Omega}_{[i]}$ is the temporal covariance matrix for the replicate $i$ and $\boldsymbol{R}_{[i]}$ the corresponding correlation matrix. For implementation, we transform the treatment covariates into dummy centred variables following @schielzethSimpleMeansImprove2010, this has the added benefit of making $\beta_{0}$ the intercept of the "average" treatment.

We will also fit the "equivalent" negative binomial model (see 2E), as a check. Intuitively, it does not allow within-patch temporal variation to be correlated across patches, so should lead to inadequate predictions of patch-level variation.

## Meta-population model

Similarly, we can write a (much simpler) model for the total metapopulation size (total number of adult females) $M$. Because there are no internal correlations to worry about here, we can use a negative binomial model here to model the within-replicate temporal variation:

$$
M_{[i,t]} \sim  {\textrm{NegBinomial}}(\lambda_{[i]}, \phi{[i]}),
$$
$$
\log(\lambda_{[i]}) = \beta_{0} + \sum_{j=1}^J({\beta_{j} \times x_{j[i]}}) + \gamma_{[i]},
$$
$$
\log(1/\phi_{[i]}) = \alpha_{0} + \sum_{j=1}^J({\alpha_{j} \times x_{j[i]}}) + \eta_{[i]},
$$
$$
\gamma_{[i]} \sim \mathrm{Normal}(0, \sigma_{\gamma}),
$$
$$
\eta_{[i]} \sim \mathrm{Normal}(0, \sigma_{\eta}).
$$  

Here $\gamma$ and $\eta$ refer to the metapopulation-level random effects, for the mean parameter and for the shape parameter, respectively. Similarly, $\beta$ and $\alpha$ refer to the fixed effects coefficients for the mean and the shape. We fit the model for the overdispersion parameter on its log-transformed inverse: the inverse transformation (as suggested in the Stan language wiki: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) is used to avoid giving too much prior weight to high overdispersion, the log transformation to keep $\phi$ estimates > 0.


## 2B - Implementation - formulas

### Patch-level model

```{r P_formulas}
P_formula <- bf(
  AFEMA~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+ # we use the dummy centred variables
    (1|METAPOP_ID/PATCH)+(0+PATCH|gr(METAPOP_ID:WEEK,by=METAPOP_ID)),
  family=poisson)
```

### Metapopulation model

```{r M_formulas}
M_formula  <- bf(
  METAPOPSUM~(is.landscape8 + is.landscape16)*is.randomized+(1|METAPOP_ID),
  nlf(shape~1/exp(loginvshape)), 
  ##we fit the model on the inverse of shape 
  ##(avoid assuming a priori that very large overdispersions _ low values of shape _ are more likely) 
  ## and we need a log transformation to keep things >0
  loginvshape~(is.landscape8 + is.landscape16)*is.randomized+(1|METAPOP_ID),
  family=negbinomial(link_shape = "identity")) #the transformation link is already "included" in the code just above

```

## 2C - Implementation - choosing priors

We then select our priors. Because the patch-level model is complex, with lots of moving parts relative to the amount of data, we can't just use "usual" weakly informative priors; we have to think a little harder to avoid a prior pushing the model towards blatantly wrong extreme values. Luckily we have independent prior data from the same species/host plant system in De Roissart *et al.* [-@deroissartDataSpatialSpatiotemporal2016; -@deroissartSpatialSpatiotemporalVariation2015], so we're going to be able to rely on explicit prior knowledge

```{r import_prior_data}
prior_data <- read_csv(here("data","prior_data.csv"))
```

(This is a copy of the relevant columns in the Dryad data in De Roissart *et al.* [-@deroissartDataSpatialSpatiotemporal2016], see there and README for details).

Densities there are given in mites per cm^2^ (our own patches are 25cm^2). We're going to use the adult female column `AvgFem`:

```{r filter_prior_data}
prior_data <- prior_data %>% 
  group_by(MP,P,Treatment,Date) %>% 
  mutate(total=AvgFem+AvgEggs+AvgMal+AvgJuv) %>% ## all the mites on the patch at time t
  group_by(MP,P,Treatment) %>%  ## metapop plus patch ID uniquely define a patch
  mutate(was_populated = mean(total)>0) %>% ## to exclude all patches that were NEVER populated
  ungroup() %>% 
  filter(was_populated==TRUE)
```

Now we're going to use these data to get very very rough prior info
```{r distri_prior_data}

log(median(prior_data$AvgFem*25))   # the \mu of a lognormal with same observed median as prior data, multiplied by patch area
log(median(prior_data$AvgFem*25*9)) # same, multiplied by total metapopulation area

log(1 + var(prior_data$AvgFem)/mean(prior_data$AvgFem)^2) # the \sigma^2 of a lognormal with same observed mean and variance
```

OK, how does this help set us reasonable priors?

First, there are five types of parameters:

- intercept for the fixed effect latent mean
- random effect standard deviation
- intercepts for the shape parameter (Metapop-model only)
- fixed effects coefficients
- correlation matrix

For the last three, we're just going to use general purpose weakly informative priors as in [@mcelreathStatisticalRethinkingBayesian2020]. But we're going to need better for the other two. A good but broad prior for the fixed effect latent mean would be "any value within the distribution of observed prior values" (so about Normal(3, 1) would work for the patch-level, about Normal(5,1) for the metapop).

For the random effect SDs, one can use the total log-scale variance in prior data as a guide, so roughly 1. The sum of replicate level variance, patch-level variance and within-patch level variance should then be around 0.9-1. So the typical prior variance for each of the 3 components should be around 0.3, leading to prior SDs around 0.5-0.55 as a pretty typical value.

(note: specifying larger priors is not dramatic and does not change outcome of among-treatment comparisons, but tends to shift all predicted means on the observed scale a bit higher than what is actually observed.)

```{r prior-setting}
P_prior <- c(
    ### PRIORS FOR THE PATCH-LEVEL MODEL
    set_prior("normal(2.8,1)", class = "Intercept"),
    set_prior("normal(0,1)", class = "b"),
    set_prior("normal(0,0.5)", class = "sd"),
    set_prior("lkj(2)", class = "cor") # mildly skeptical of very high correlations (positive or negative)
  )

M_prior <- c(
      ### PRIORS FOR THE METAPOP LEVEL MODEL
    set_prior("normal(5,1)", class = "Intercept"),
    set_prior("normal(0,1)", class = "b"),
    set_prior("normal(0,1)", class="b",nlpar="loginvshape"),
    set_prior("normal(0,1)", class = "sd", nlpar="loginvshape"),
    set_prior("normal(0,0.5)", class = "sd")
)
```

## 2D - Implementation - fitting

We can now fit the models based on the choices we made earlier

```{r P_model_fitting}
if(file.exists(here("R_output","P_mod.Rdata"))){
  # this if-else statement is avoid re-fitting a model if there is already one existing in R_output
  # to override, re-run the model and re-save manually by selecting only the relevant code lines (or delete the Rdata object before relaunching this code chunk)
  load(here("R_output","P_mod.Rdata"))
  }else
    {

P_mod <- brm(formula=P_formula, prior=P_prior, data = P_data, 
           iter=N_iter, warmup=N_warmup, chains=N_chains,
           seed=42, control=list(adapt_delta=0.9,max_treedepth=15),
           backend="cmdstanr")
save(list=c("P_mod"), file=here("R_output","P_mod.Rdata"))
}
```

(Note that it can take some time to finish.)

```{r M_model_fitting}
if(file.exists(here("R_output","M_mod.Rdata"))){
  load(here("R_output","M_mod.Rdata"))
  }else{
M_mod <- brm(formula=M_formula, prior=M_prior, data = M_data, 
           iter = N_iter, warmup = N_warmup, chains = N_chains, seed = 42,
           control = list(adapt_delta = 0.9),
           backend= "cmdstanr")

save(list=c("M_mod"), file=here("R_output","M_mod.Rdata"))
}

## with rstan as backend, the model *may* throw some some rhats=NA warnings on exit
## they can (for once) be safely ignored: related issue: https://github.com/paul-buerkner/brms/issues/865
## these warnings don't pop up when cmdstanr is used as backend 
## (other "normal" warnings will appear if needed (e.g during a test run with low iters) with both backends)
## or when doing summary(mod)
## you can check that only parameters that should be constant give NA warnings through rhat(mod)
## and quickly look at the rhats:
## hist(rhat(mod));abline(v=1.01,col="red")
```


```{r model_summary}
summary(P_mod)  #look at Rhat and ESS both tail and bulk
```

You also need to check that the model can reproduce the data well, through posterior predictive checks [@gabryVisualizationBayesianWorkflow2019]
(here, given we have observation-level random effects, it should be OK)

```{r model_ppchecks}
pp_check(P_mod, nsamples = 20)
## can the model generate the observed data? (there are many ppcheck possibles, see the help of the bayesplot package)
## for P_mod, basic pp_checks will return perfect fits, since there are obs-level random effects
```

## 2E - couldn't the patch-level model be simpler?

One may wonder why go that far for the patch-level model. Surely one could simply use a negative binomial model similar to the metapopulation-level one, just one level lower. Not quite. First, we can't gather the variables needed to estimate alpha, beta and gamma variabilities from such a model. And second, such a model assumes that within-patch temporal variation is stochastic, and ignores across-patch temporal covariation. Let's have a quick look:

```{r P_model_fitting_alternate}
### adapted formula
P_formula2 <- bf(
  AFEMA~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+
    (1|METAPOP_ID/PATCH),
  nlf(shape~1/exp(loginvshape)),
  loginvshape~(is.local5 + is.local3 + is.landscape8 + is.landscape16)*is.randomized+
      (1|METAPOP_ID/PATCH),
  family=negbinomial(link_shape="identity"))

### adapted priors
P_prior2 <- c(
  ### UPDATED PRIORS FOR THE PATCH LEVEL MODEL
  set_prior("normal(2.8,1)", class = "Intercept"),
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class="b",nlpar="loginvshape"),
  set_prior("normal(0,1)", class = "sd", nlpar="loginvshape"),
  set_prior("normal(0,0.5)", class = "sd")
)

### adapted model
if(file.exists(here("R_output","P_mod_alt.Rdata"))){
    load(here("R_output","P_mod_alt.Rdata"))
  }else{
P_mod2 <- brm(formula=P_formula2, prior=P_prior2, data = P_data, 
             iter=N_iter,warmup=N_warmup,chains=N_chains,
             seed=42, control=list(adapt_delta=0.9,max_treedepth=15),
             backend="cmdstanr")
save(list=c("P_mod2"), file=here("R_output","P_mod_alt.Rdata"))
  }
```


```{r P_model_fitting_alternate_checks}
### let's build some residuals (ratio between observed values and predicted patch-level means)
### only the posterior mean, though, not the full posterior
P_data$alt_residuals=(P_data$AFEMA/fitted(P_mod2, re_formula=~(1|METAPOP_ID/PATCH))[,1])

ggplot(P_data) +
  geom_boxplot(aes(factor(WEEK),alt_residuals))+
  facet_wrap(~METAPOP_ID)+geom_hline(yintercept=1,col="red")+
  coord_cartesian(ylim=c(0,4)) #cropping outlier dots to better see the boxes
## clear evidence of week-to-week metapopulation-wide fluctuations that are not accounted for by the negative binomial model
## = patches do not vary independently, contra assumptions in negbin model

ggplot(P_data) +
  geom_boxplot(aes(factor(PATCH),alt_residuals))+
  facet_wrap(~METAPOP_ID)+geom_hline(yintercept=1,col="red")+
  coord_cartesian(ylim=c(0,4))
## while patch-to-patch overall spatial variations are more-or-less* well accounted for, as one would expect from model structure
## *if anything there is actually a bias toward the negbin model slightly overestimating true patch values, actually

```

It's fairly easy to see that the negative binomial model for patches clearly misses the fact that the patches in a given metapopulation vary somewhat synchronously from week to week, and is thus incomplete.


```{r waic_loo}
waic(P_mod)
waic(P_mod2)

loo(P_mod)
loo(P_mod2)
```

There are **a lot** of potential issues in my opinion with using WAIC or LOO-PSIS for comparing the two models:
- first, there are many "bad" observations re: pareto_k, so we should probably use K-fold cross validation, but that would involve refitting the models K times, so add a lot of computing time
- second, even then, given one model has observation-level random effects, so literally predicts every point, this feels a little weird and cheating (though this is penalized by greatly increasing the effective number of parameters p_waic or p_loo)
But putting that aside it looks like, even after penalizing for the hundreds of underlying parameters added by the observation-level effect, the Poisson+OLRE model is indeed better than the negative binomial one

# Part 3 - Obtaining key metrics from model output

Once we have fitted and chosen a model, it's time to extract what we need from it. 

There's going to be a lot of variables, so we're gonna keep following our naming convention to keep things clear:

- columns or objects prefixed by `P_` refer to the patch by patch model
- ...prefixed by `M_` to the metapop-level model

## Extraction

Let's start by extracting the total variance-covariance info:

```{r extract_P_vcv}
memory.limit(40000) #### if needed, a little memory boost above defaults to handle the full vcv matrices in final model

P_full_vcv <- VarCorr(P_mod, summary = FALSE)
```

Then from it we extract the SDs for the patch and replicate level, as well as the VCV matrices for temporal variation (then we delete the big total covariance object, to save room in memory):

```{r extract_P_vcv2}
P_sd_metapop<-P_full_vcv$`METAPOP_ID`$sd

P_sd_patch<-P_full_vcv$`METAPOP_ID:PATCH`$sd

P_vcv_time_global <- P_full_vcv$`METAPOP_ID:WEEK`$cov

rm(P_full_vcv) ## we remove as VERY large and not needed anymore
gc() #garbage collection
```

The `P_vcv_time_global` object contains all the (co)variances in a single matrix, including the meaningless (and 0 by construction) covariances between the patches from a replicate and the patches from another, so it is very very sparse. Much better and practical to convert it into a list of the actual smaller, replicate-level, covariance matrix. We create an object where a row = a combination replicate x posterior iteration, and then use the information contained in matrices row-column names to return the right submatrix for each row, nothing more:

```{r P_vcv_split}
P_vcv_time_latent<-P_data %>% 
  select(METAPOP_ID,LENGTH,SHUFFLE) %>% 
  expand_grid(.iteration = 1:(N_chains * (N_iter-N_warmup))) %>% 
  distinct() %>% 
  mutate(P_vcv_time_latent=map2(.x=.iteration,.y=METAPOP_ID,
                                .f=function(iter=.x,metapop=.y,source=P_vcv_time_global){
                                  include <- colnames(source)
                                  include <- include[str_detect(include, pattern = metapop)]
                                  return(source[iter,include,include])
                                }))
```

We then need to estimate the latent patch-level intercepts. For our purpose, this means including all fixed and random effects specifying differences among patches (leaving out only within patch effects). We then rearrange this so that predicted latent intercepts for all patches in a replicate x iteration combination are as a vector in one list, i.e. a format compatible with the one we chose for the covariance matrices:

```{r create_latent_intercepts}
P_latent_intercepts <- P_data %>% 
  select(METAPOP_ID,SHUFFLE,LENGTH,local_connectedness,PATCH, 
         is.local5, is.local3, is.landscape8, is.landscape16, is.randomized) %>% 
  distinct() %>% 
  add_fitted_draws(P_mod,re_formula=~(1|METAPOP_ID/PATCH),scale="linear") %>% 
  ungroup() 

P_latent_intercepts <- P_latent_intercepts %>% 
  select(METAPOP_ID,SHUFFLE,LENGTH,PATCH,.iteration=.draw,.value) %>% 
  pivot_wider(values_from=.value,names_from=PATCH) %>% 
  group_by(METAPOP_ID,SHUFFLE,LENGTH,.iteration) %>% 
  nest(data=c(P11,P12,P13,P21,P22,P23,P31,P32,P33)) %>% 
  rename(P_latent_intercept="data") %>% 
  mutate(P_latent_intercept=map(.x=P_latent_intercept,
                                .f=~.x %>% unlist()))
```

We can then merge the "latent intercept" table and the covariance table. To have all information from the model in one convenient table, we also add the replicate and among patch-variances (not used in building the predictions themselves, since their variances is already added in the latent intercept):

```{r merge-coefficients-tables}
P_tab <- P_latent_intercepts %>% 
  left_join(P_vcv_time_latent) 

P_vars<-tibble(.iteration=1:8000,
               P_var_metapop=P_sd_metapop[,1]^2,
               P_var_patch=P_sd_patch[,1]^2)

P_tab<-P_tab %>% 
  left_join(P_vars)
```

## Estimation of key quantities

We first use information on patch-level fixed effects and patch-level temporal variances to estimate data-scale patch-level means. Because we allow within-patch variance to vary among replicates (and so among treatments), we can't infer anything about *mean* differences among treatments from fixed effects only, as the data-scale mean depends on both latent mean and latent within-patch variance [see @villemereuilGeneralMethodsEvolutionary2016]. So we need to first estimate patch means from latent means and variances, and then average across patches within replicates. As a Poisson with observation-level random effect can be seen as a "log-normal Poisson", the formula for the observed mean is the same once you know the latent mean and the latent variance.

```{r P_means}
P_tab <- P_tab  %>%  
  mutate(P_pred = map2( ## patch by patch patch-level average
    .x = P_latent_intercept, .y = P_vcv_time_latent,
    .f = ~ exp(.x + diag(.y)/2)  ## analytic form for the Poisson model, see e.g. annex villemereuil
  )) %>% 
  ### we then average within metapops:
  mutate(P_mean_all = map(.x = P_pred, .f = ~mean(.x))) %>% 
  mutate(P_mean_corners = map(.x = P_pred, .f = ~mean(.x[c(1,3,7,9)]))) %>% 
  mutate(P_mean_center = map(.x = P_pred, .f = ~mean(.x[c(5)]))) %>% 
  mutate(P_mean_sides = map(.x = P_pred, .f = ~mean(.x[c(2,4,6,8)]))) %>%
  unnest(c(P_mean_all, P_mean_corners,P_mean_center,P_mean_sides)) 
```

Now, to estimate $\alpha$, $\beta$ and $\gamma$ variabilities (and the unevenness metric), we need to get the temporal variance-covariance matrices from the latent scale to the data scale [see @villemereuilGeneralMethodsEvolutionary2016]. Ideally we'd use the `QGmv...` functions in the `QGglmm` package, but they become terribly inefficient in memory use once the multivariate model has more than 5 variables (and we have 9). It's because the multivariate functions integrate with `cubature` rather than using the closed form version, (if I understood correctly) because they have to work even for cases where the different variables are from different families. But we're lucky because our 9 variables have the same family, so it's actually very easy to do it manually using the closed form formulas in the source papers!! 

The first step is to estimate a $\psi$ matrix, a diagonal matrix containing for each variable its $\psi$ value. And actually, we've already estimated it, since for a Poisson model, $\psi$ is the predicted mean (i.e. `P_pred` for us). So we just have to diagonalise it. Then we can use it to convert the latent scale temporal VCV to the observed scale, and finally use these VCVs and the predicted means to estimate the $\alpha$, $\beta$ and $\gamma$ variabilities:

```{r P_variabilities}
P_tab <- P_tab %>% 
  mutate(P_psi = map(.x = P_pred, .f = function(.x){diag(.x)})) %>% 
  mutate(P_vcv_time_obs = map2(.x = P_psi, .y= P_vcv_time_latent,
                                .f = ~ (.x %*% .y %*% t(.x))
  )) %>% 
  mutate( 
    P_alpha = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% alpha_wang_loreau(varcorr=., means = .y)),
    P_gamma = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% gamma_wang_loreau(varcorr=., means = .y)),
    P_uneven = map2(.x = P_vcv_time_obs, .y = P_pred, .f = ~.x %>% uneven_wang_loreau(varcorr=., means = .y))
  ) %>% 
  unnest(cols=c(P_alpha,P_gamma,P_uneven)) %>% 
  mutate(
    P_beta1 = P_alpha / P_gamma, P_phi = P_gamma/P_alpha, P_beta2 = P_alpha - P_gamma
  )
```

We can then export a clean data table to not have to re-do all of this everytime, if we just want to change things in the plots and comparisons below:

```{r for_export}

P_tab_plots <- P_tab %>% ungroup() %>% select(-c(P_vcv_time_latent, P_latent_intercept, P_pred, P_psi, P_vcv_time_obs))

write_csv(x = P_tab_plots, file = here("R_output","P_tab.csv"))
```


# Part 4- Figures and comparisons [BIG WORK IN PROGRESS]

## Data preparation

We re-import the final table of the previous step, and we update the `LENGTH`/metapopulation connectedness variable so that the order makes sense (increasing connectedness):

```{r reimport}
P_tab_plots <- read_csv(here("R_output","P_tab.csv")) %>% 
  mutate(
    LENGTH=fct_relevel(
      factor(LENGTH),
      "16 cm (low)","8 cm (medium)","4 cm (high)")
  )
```

And we need to summarize the observed values to display them along the posterior when relevant:

```{r obs_summaries}
P_obsmeans <- P_data %>%
  group_by(SHUFFLE,LENGTH,METAPOP_ID,PATCH, local_connectedness) %>%
  summarise(P_mean_obs=mean(AFEMA), P_var_obs=var(AFEMA)) %>% 
  ungroup()%>% 
  mutate(
    LENGTH=fct_relevel(
      factor(LENGTH),
      "16 cm (low)","8 cm (medium)","4 cm (high)"),
    local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )
  )
```

Before doing the figures, we also need to create color palettes that'll be used throughout:

```{r figure_palettes}
paletteLENGTH <- c("#D55E00", "#E69F00", "#F0E442") #for figures 2 to 4
paletteLOCAL <-  c("#e66101", "#fdb863", "#5e3c99") #for figure 5
```

## Patch-level means, additive effects

### Figure 1

We average the replicate-specific predictions across treatments, then we plot these posteriors along with the corresponding observed means. The observed means are plotted so that the less precise estimates (higher mean-standardised variance) are more transparent. (three subplots, one per treatment and connectedness variable):

```{r fig1}
p1<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, 
              aes(x=P_mean_obs,y=SHUFFLE,alpha=1/(P_var_obs^2/P_mean_obs^2)),
              col="grey60", height=0.2,size=3) +
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",
               .width=c(0.01,0.95),slab_alpha=0.5,
               point_interval=mean_hdi)+
  scale_x_continuous("")+
  scale_y_discrete("Randomization?")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p2 <- P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_mean_all))  %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, 
              aes(x=P_mean_obs,y=LENGTH,alpha=1/(P_var_obs^2/P_mean_obs^2)),
              col="grey60",height=0.2,size=3) +
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",
               .width=c(0.01,0.95),slab_alpha=0.5,
               point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("")+
  scale_y_discrete("Metapopulation connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p3 <- P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),
               names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "P_mean_center", 
                                          `side (medium)` = "P_mean_sides", 
                                          `corner (low)`="P_mean_corners")) %>%
  mutate(local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )) %>% 
  group_by(.iteration, local_connectedness) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, 
              aes(x=P_mean_obs,y=local_connectedness,
                  alpha=1/(P_var_obs^2/P_mean_obs^2)),
              col="grey60", height=0.2,size=3) +
  stat_halfeye(aes(x=mean,y=local_connectedness, fill=local_connectedness),
               orientation="horizontal",
               .width=c(0.01,0.95),slab_alpha=0.5,
               point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLOCAL)+
  scale_x_continuous("temporal mean of patch population size (adult females)")+
  scale_y_discrete("Local connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p1/p2/p3

```

### Pairwise comparisons

We can then compare the posteriors for each subplot, to see how they are different. (Note: we do multiplicative comparisons _ ratios _ throughout since they seem more appropriate to count data then differences)

```{r pairwise_comp_fig1}

### pairwise for randomization
P_tab_plots %>% 
  group_by(.iteration, SHUFFLE)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE, fun=`/`) %>% 
  mean_hdi()

### pairwise for metapopulation connectedness
P_tab_plots %>% 
  group_by(.iteration, LENGTH)  %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH, fun=`/`) %>% 
  mean_hdi()

### pairwise for local connectedness
P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (8 links)` = "P_mean_center", 
                                          `side (5 links)` = "P_mean_sides", 
                                          `corner (3 links)`="P_mean_corners")) %>% 
  group_by(.iteration, local_connectedness) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
    ungroup() %>% 
  compare_levels(variable=mean, by=local_connectedness, fun=`/`) %>% 
  mean_hdi()
```

## Metapopulation variability metrics

### Figure 2

```{r P_vars}
p1<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_x_continuous("")+
  scale_y_discrete("Randomization?")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.8,1.4))

p2<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_alpha))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean ", alpha, " variability")))+
  scale_y_discrete("Metapopulation connectedness")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.8,1.4))

p3<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.20,0.35))

p4<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_phi))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean spatial synchrony (", phi1, " = ", 1/beta,")")))+  #phi1 to get the cursive
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.20,0.35))

p5<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.15,0.45))

p6<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_gamma))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous(expression(paste("mean ", gamma, " variability")))+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0.15,0.45))

p7<-P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=SHUFFLE, fill=SHUFFLE),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_x_continuous("")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+
  coord_cartesian(xlim=c(0,1.5))

p8<-P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>% 
  summarise(mean = mean(P_uneven))  %>% 
  ggplot()+
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("mean spatial unevenness")+
  scale_y_discrete("")+
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(0,1.5))

(p1/p2)|(p3/p4)|(p5/p6)|(p7/p8)

```

### Pairwise comparisons

```{r}
P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, SHUFFLE) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=SHUFFLE,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_alpha)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_phi)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_gamma)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()

P_tab_plots %>% 
  group_by(.iteration, LENGTH) %>%  
  summarise(mean = mean(P_uneven)) %>% 
  ungroup() %>% 
  compare_levels(variable=mean, by=LENGTH,fun=`/`) %>% 
  mean_hdi()
```

## Patch-level means, interactive effects

### Figure 3

```{r P_size_inter}
p1<-P_tab_plots %>% 
  group_by(.iteration, LENGTH, SHUFFLE) %>% 
  summarise(mean = mean(P_mean_all)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=LENGTH,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60",height=0.2,size=3) +
  stat_halfeye(aes(x=mean,y=LENGTH, fill=LENGTH),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLENGTH)+
  scale_x_continuous("")+
  scale_y_discrete("Metapopulation connectedness")+
  facet_wrap(~SHUFFLE) +
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p2 <- P_tab_plots %>% 
  pivot_longer(cols=c(P_mean_center,P_mean_sides,P_mean_corners),names_to="local_connectedness",values_to="P_mean_local") %>% 
  mutate(local_connectedness = fct_recode(factor(local_connectedness),
                                          `center (high)` = "P_mean_center", 
                                          `side (medium)` = "P_mean_sides", 
                                          `corner (low)`="P_mean_corners")) %>% 
    mutate(local_connectedness=fct_relevel(
      factor(local_connectedness),
      "corner (low)","side (medium)" ,"center (high)"
    )) %>% 
  group_by(.iteration, local_connectedness, SHUFFLE) %>% 
  summarise(mean = mean(P_mean_local)) %>% 
  ggplot()+
  geom_jitter(data=P_obsmeans, aes(x=P_mean,y=local_connectedness,alpha=(1/(P_se^2/P_mean^2))),
              col="grey60",height=0.2,size=3) +
  stat_halfeye(aes(x=mean,y=local_connectedness, fill=local_connectedness),
               orientation="horizontal",.width=c(0.01,0.95),slab_alpha=0.5,point_interval=mean_hdi)+
  scale_fill_manual(values=paletteLOCAL)+
  scale_x_continuous("mean patch population size (adult females)")+
  scale_y_discrete("Local connectedness")+
  facet_wrap(~SHUFFLE) +
  cowplot::theme_half_open(11) +
  cowplot::background_grid(colour.major = "grey95", colour.minor = "grey95") +
  theme(legend.position = "none")+ 
  coord_cartesian(xlim=c(5,50))

p1/p2
```

### Pairwise comparisons

to do

# References




